{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cadb87",
   "metadata": {},
   "source": [
    "# [GD-07] BERT Pretrained Model\n",
    "\"Going Deeper Node 14. Making BERT Pretrained model\" / 2022. 04. 21 (Mon) 이형주\n",
    "\n",
    "## Contents\n",
    "---\n",
    "- **1. Environment Setup**\n",
    "- **2. Modeling**\n",
    "- **3. Project Retrospective**\n",
    "\n",
    "## Rubric 평가기준\n",
    "---\n",
    "\n",
    "|  평가문항  |  상세기준  |\n",
    "|:---------|:---------|\n",
    "|1. 한글 코퍼스를 가공하여 BERT pretrain용 데이터셋을 잘 생성하였다.|MLM, NSP task의 특징이 잘 반영된 pretrain용 데이터셋 생성과정이 체계적으로 진행되었다.\n",
    "|2. 구현한 BERT 모델의 학습이 안정적으로 진행됨을 확인하였다.|학습진행 과정 중에 MLM, NSP loss의 안정적인 감소가 확인되었다.\n",
    "3. 1M짜리 mini BERT 모델의 제작과 학습이 정상적으로 진행되었다.|학습된 모델 및 학습과정의 시각화 내역이 제출되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd8a5a",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f017e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0870219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/bert_pretrain/data/kowiki.txt --model_prefix=ko_8000 --vocab_size=8007 --model_type=bpe --max_sentence_length=999999 --pad_id=0 --pad_piece=[PAD] --unk_id=1 --unk_piece=[UNK] --bos_id=2 --bos_piece=[BOS] --eos_id=3 --eos_piece=[EOS] --user_defined_symbols=[SEP],[CLS],[MASK]\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/bert_pretrain/data/kowiki.txt\n",
      "  input_format: \n",
      "  model_prefix: ko_8000\n",
      "  model_type: BPE\n",
      "  vocab_size: 8007\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 999999\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: [SEP]\n",
      "  user_defined_symbols: [CLS]\n",
      "  user_defined_symbols: [MASK]\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: [UNK]\n",
      "  bos_piece: [BOS]\n",
      "  eos_piece: [EOS]\n",
      "  pad_piece: [PAD]\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/bert_pretrain/data/kowiki.txt\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(117) LOG(WARNING) Too many sentences are loaded! (2451287), which may slow down training.\n",
      "trainer_interface.cc(119) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(122) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 2451287 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [PAD]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [UNK]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [BOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [EOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [SEP]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [CLS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [MASK]\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=287452241\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=4411\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 2450254 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 2450254\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 7050692\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1781571 min_freq=424\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=576838 size=20 all=581927 active=38577 piece=▁아\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=390836 size=40 all=591445 active=48095 piece=▁유\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=297873 size=60 all=601378 active=58028 piece=에는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=244712 size=80 all=609974 active=66624 piece=▁성\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=194372 size=100 all=616449 active=73099 piece=까지\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=193674 min_freq=462\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=176838 size=120 all=625299 active=38770 piece=▁우\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=154294 size=140 all=632274 active=45745 piece=▁파\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=140625 size=160 all=639734 active=53205 piece=00\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=125983 size=180 all=645481 active=58952 piece=▁요\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=114855 size=200 all=649839 active=63310 piece=리아\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=114086 min_freq=457\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=106760 size=220 all=657338 active=39316 piece=▁같은\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=100770 size=240 all=662564 active=44542 piece=▁왕\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=96037 size=260 all=670536 active=52514 piece=▁목\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=88392 size=280 all=675441 active=57419 piece=▁f\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=81678 size=300 all=681701 active=63679 piece=▁선수\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=80870 min_freq=446\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=77144 size=320 all=686930 active=39163 piece=▁때문에\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=73218 size=340 all=691000 active=43233 piece=▁조선\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=68829 size=360 all=695717 active=47950 piece=▁천\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=64009 size=380 all=700839 active=53072 piece=▁196\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=60953 size=400 all=706675 active=58908 piece=▁돌\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=60762 min_freq=435\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=58542 size=420 all=711350 active=39712 piece=▁다시\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=55779 size=440 all=715377 active=43739 piece=▁K\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=52528 size=460 all=721839 active=50201 piece=▁모두\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=49784 size=480 all=727356 active=55718 piece=▁히\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=47637 size=500 all=733038 active=61400 piece=▁전쟁\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=47558 min_freq=423\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=45919 size=520 all=738287 active=41703 piece=▁있어\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=44025 size=540 all=743886 active=47302 piece=▁중심\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=42378 size=560 all=748357 active=51773 piece=▁N\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=40922 size=580 all=752114 active=55530 piece=▁H\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=39922 size=600 all=755142 active=58558 piece=le\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=39768 min_freq=414\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=38924 size=620 all=761204 active=43650 piece=▁검\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=37771 size=640 all=767376 active=49822 piece=란드\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=36292 size=660 all=772837 active=55283 piece=정을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=35134 size=680 all=778715 active=61161 piece=▁설립\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=34016 size=700 all=783719 active=66165 piece=▁역사\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=34003 min_freq=400\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=33144 size=720 all=787243 active=42507 piece=▁만들어\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=32383 size=740 all=791538 active=46802 piece=▁시간\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=31645 size=760 all=795131 active=50395 piece=▁측\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=30941 size=780 all=798845 active=54109 piece=과의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=30041 size=800 all=803050 active=58314 piece=도는\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=30023 min_freq=392\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=29448 size=820 all=808546 active=45099 piece=▁난\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=28836 size=840 all=813895 active=50448 piece=▁21\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=28270 size=860 all=818654 active=55207 piece=▁찾\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=27299 size=880 all=824625 active=61177 piece=되지\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=26862 size=900 all=828285 active=64837 piece=하자\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=26833 min_freq=381\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=26060 size=920 all=832595 active=45199 piece=부에\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=25504 size=940 all=838824 active=51428 piece=수의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=24726 size=960 all=843322 active=55926 piece=▁남아\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=24083 size=980 all=848416 active=61020 piece=▁않는다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=23647 size=1000 all=853601 active=66205 piece=인민\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=23602 min_freq=368\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=23176 size=1020 all=859529 active=48367 piece=▁마지\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=22849 size=1040 all=863129 active=51967 piece=▁시리즈\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=22293 size=1060 all=868678 active=57516 piece=제로\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=21851 size=1080 all=873075 active=61913 piece=시의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=21377 size=1100 all=878277 active=67115 piece=해야\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=21369 min_freq=355\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=21043 size=1120 all=882003 active=47093 piece=▁녹\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=20625 size=1140 all=886266 active=51356 piece=▁인구는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=20185 size=1160 all=889382 active=54472 piece=ac\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19762 size=1180 all=894738 active=59828 piece=인은\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19447 size=1200 all=899565 active=64655 piece=50\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=19432 min_freq=347\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19118 size=1220 all=903215 active=48471 piece=광역\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=18777 size=1240 all=908007 active=53263 piece=수는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=18396 size=1260 all=913268 active=58524 piece=인민공\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=18040 size=1280 all=917389 active=62645 piece=▁긴\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=17659 size=1300 all=922177 active=67433 piece=▁전통\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=17629 min_freq=335\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=17404 size=1320 all=926354 active=50086 piece=▁망\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=17093 size=1340 all=929494 active=53226 piece=단의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16931 size=1360 all=934269 active=58001 piece=▁인간\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16719 size=1380 all=939872 active=63604 piece=▁바로\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16586 size=1400 all=943447 active=67179 piece=▁탑\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=16584 min_freq=327\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16331 size=1420 all=947545 active=51069 piece=▁태양\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16086 size=1440 all=952041 active=55565 piece=▁경기에서\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=15873 size=1460 all=957568 active=61092 piece=▁동일\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=15577 size=1480 all=962681 active=66205 piece=드를\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=15352 size=1500 all=966310 active=69834 piece=▁뮤\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=15328 min_freq=318\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=15103 size=1520 all=968851 active=50741 piece=id\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14971 size=1540 all=972545 active=54435 piece=▁옥\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14723 size=1560 all=977306 active=59196 piece=비전\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14495 size=1580 all=982182 active=64072 piece=▁대부분의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14292 size=1600 all=985970 active=67860 piece=▁갑\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=14280 min_freq=311\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14118 size=1620 all=990010 active=53173 piece=회는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13982 size=1640 all=993514 active=56677 piece=▁사고\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13729 size=1660 all=997224 active=60387 piece=cm\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13567 size=1680 all=1001600 active=64762 piece=▁뛰어\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13433 size=1700 all=1006574 active=69736 piece=▁더욱\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=13420 min_freq=303\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13135 size=1720 all=1011076 active=54820 piece=지역\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12940 size=1740 all=1015431 active=59175 piece=▁선언\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12821 size=1760 all=1020339 active=64083 piece=▁어려\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12640 size=1780 all=1023221 active=66965 piece=▁칭\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12505 size=1800 all=1028177 active=71921 piece=▁옛\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=12503 min_freq=295\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12409 size=1820 all=1033793 active=56931 piece=ce\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12253 size=1840 all=1037183 active=60321 piece=▁그들의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12091 size=1860 all=1041900 active=65038 piece=▁단체\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11934 size=1880 all=1045379 active=68517 piece=▁예술\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11799 size=1900 all=1048007 active=71145 piece=라의\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=11799 min_freq=288\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11693 size=1920 all=1050313 active=54358 piece=▁불구하고\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11594 size=1940 all=1053703 active=57748 piece=▁분리\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11496 size=1960 all=1057998 active=62043 piece=▁사이의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11381 size=1980 all=1063418 active=67463 piece=단이\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11289 size=2000 all=1067885 active=71930 piece=im\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=11277 min_freq=281\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11133 size=2020 all=1071888 active=57167 piece=▁등과\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10994 size=2040 all=1077153 active=62432 piece=법을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10895 size=2060 all=1081334 active=66613 piece=▁괴\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10776 size=2080 all=1084885 active=70164 piece=러스\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10692 size=2100 all=1089330 active=74609 piece=▁깨\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=10688 min_freq=274\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10597 size=2120 all=1091576 active=56585 piece=릭터\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10496 size=2140 all=1095180 active=60189 piece=▁확장\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10396 size=2160 all=1100510 active=65519 piece=었던\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10271 size=2180 all=1104334 active=69343 piece=▁남부\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10143 size=2200 all=1108628 active=73637 piece=▁스포츠\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=10136 min_freq=268\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10057 size=2220 all=1111848 active=58430 piece=▁이외\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9957 size=2240 all=1115235 active=61817 piece=▁서식\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9848 size=2260 all=1120216 active=66798 piece=▁작용\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9732 size=2280 all=1124443 active=71025 piece=▁이야기\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9655 size=2300 all=1129106 active=75688 piece=▁성립\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=9653 min_freq=261\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9570 size=2320 all=1132064 active=59284 piece=▁떨어진\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9473 size=2340 all=1135883 active=63103 piece=시를\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9352 size=2360 all=1139681 active=66901 piece=▁히로\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9289 size=2380 all=1142636 active=69856 piece=▁부정\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9219 size=2400 all=1147544 active=74764 piece=▁2020\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=9217 min_freq=256\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9137 size=2420 all=1150230 active=60032 piece=▁상징\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9028 size=2440 all=1153890 active=63692 piece=▁1950\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8965 size=2460 all=1157151 active=66953 piece=▁표시\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8889 size=2480 all=1162268 active=72070 piece=▁사용된다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8828 size=2500 all=1165804 active=75606 piece=▁아일랜드\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=8826 min_freq=251\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8738 size=2520 all=1169223 active=61615 piece=▁동생\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8694 size=2540 all=1173546 active=65938 piece=ie\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8641 size=2560 all=1176802 active=69194 piece=▁경상북도\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8570 size=2580 all=1180886 active=73278 piece=▁계승\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8489 size=2600 all=1184747 active=77139 piece=▁1985\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=8487 min_freq=246\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8434 size=2620 all=1188193 active=62666 piece=력의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8345 size=2640 all=1191415 active=65888 piece=인지\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8270 size=2660 all=1196857 active=71330 piece=▁달성\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8223 size=2680 all=1200791 active=75264 piece=▁충돌\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8139 size=2700 all=1202837 active=77310 piece=ak\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=8133 min_freq=241\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8061 size=2720 all=1205959 active=63019 piece=▁2,\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8008 size=2740 all=1208089 active=65149 piece=)\"\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7956 size=2760 all=1210710 active=67770 piece=▁둘러\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7894 size=2780 all=1213790 active=70850 piece=워드\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7817 size=2800 all=1216934 active=73994 piece=▁즐\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=7817 min_freq=237\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7753 size=2820 all=1220480 active=64363 piece=석을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7703 size=2840 all=1224543 active=68426 piece=인들은\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7656 size=2860 all=1229199 active=73082 piece=▁MBC\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7587 size=2880 all=1232852 active=76735 piece=산의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7529 size=2900 all=1238593 active=82476 piece=생활\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=7526 min_freq=232\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7474 size=2920 all=1244080 active=66850 piece=년대에\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7408 size=2940 all=1248652 active=71422 piece=▁아우\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7352 size=2960 all=1252722 active=75492 piece=▁대회에서\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7304 size=2980 all=1256113 active=78883 piece=프가\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7254 size=3000 all=1258582 active=81352 piece=가가\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=7254 min_freq=227\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7200 size=3020 all=1262333 active=66190 piece=▁출발\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7130 size=3040 all=1266484 active=70341 piece=주가\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7087 size=3060 all=1270279 active=74136 piece=▁경우도\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7049 size=3080 all=1274031 active=77887 piece=▁전개\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7000 size=3100 all=1277747 active=81603 piece=대전\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=6999 min_freq=222\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6941 size=3120 all=1279813 active=65635 piece=ell\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6902 size=3140 all=1283065 active=68887 piece=트라\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6853 size=3160 all=1286899 active=72721 piece=음악\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6817 size=3180 all=1290334 active=76156 piece=▁성우\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6770 size=3200 all=1293860 active=79682 piece=▁구역\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=6770 min_freq=219\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6707 size=3220 all=1297078 active=67788 piece=일보\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6660 size=3240 all=1301117 active=71827 piece=트에\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6617 size=3260 all=1304163 active=74873 piece=▁라이브\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6579 size=3280 all=1308287 active=78997 piece=▁이끄는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6549 size=3300 all=1311934 active=82644 piece=▁1976\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=6547 min_freq=214\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6517 size=3320 all=1314193 active=67834 piece=▁방식으로\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6480 size=3340 all=1317604 active=71245 piece=▁설치되어\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6448 size=3360 all=1321261 active=74902 piece=...\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6408 size=3380 all=1323725 active=77366 piece=초등학교는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6385 size=3400 all=1327377 active=81018 piece=▁고구려\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=6380 min_freq=211\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6352 size=3420 all=1329508 active=68387 piece=▁이스라엘\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6312 size=3440 all=1333149 active=72028 piece=▁초반\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6272 size=3460 all=1336419 active=75298 piece=싱턴\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6227 size=3480 all=1339417 active=78296 piece=오프\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6191 size=3500 all=1341801 active=80680 piece=시와\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=6191 min_freq=208\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6155 size=3520 all=1344660 active=69389 piece=▁잭\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6115 size=3540 all=1347440 active=72169 piece=지면서\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6075 size=3560 all=1350874 active=75603 piece=세계\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6049 size=3580 all=1355093 active=79822 piece=받은\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: ko_8000.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: ko_8000.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "prefix = 'ko_8000'\n",
    "vocab_size = 8000\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa09ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/models'\n",
    "model_k_8000_dir = os.getenv('HOME')+'/aiffel/ko_8000.model'\n",
    "\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(model_k_8000_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb53449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수 토큰 7개를 제외한 나머지 token list 추가\n",
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "    if not vocab.is_unknown(id):\n",
    "        vocab_list.append(vocab.id_to_piece(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9deab44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "tokens_org = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "print(tokens_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99650e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 token 15%를 마스킹\n",
    "mask_cnt = int((len(tokens_org) - 3) * 0.15)\n",
    "mask_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e564798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4] ['▁추', '적', '추', '적']\n",
      "[5, 6] ['▁비', '가']\n",
      "[7, 8] ['▁내', '리는']\n",
      "[9, 10, 11] ['▁날', '이었', '어']\n",
      "[12, 13, 14] ['▁그', '날', '은']\n",
      "[15, 16, 17] ['▁', '왠', '지']\n",
      "[18, 19, 20] ['▁손', '님', '이']\n",
      "[21, 22] ['▁많', '아']\n",
      "[23] ['▁첫']\n",
      "[24, 25] ['▁번', '에']\n",
      "[26, 27] ['▁삼', '십']\n",
      "[28] ['▁전']\n",
      "[29, 30, 31] ['▁둘', '째', '번']\n",
      "[32, 33] ['▁오', '십']\n",
      "[34] ['▁전']\n",
      "[35, 36, 37] ['▁오', '랜', '만에']\n",
      "[38, 39, 40] ['▁받아', '보', '는']\n",
      "[41] ['▁십']\n",
      "[42, 43, 44] ['▁전', '짜', '리']\n",
      "[45, 46, 47] ['▁백', '통', '화']\n",
      "[48, 49, 50] ['▁서', '푼', '에']\n",
      "[52, 53, 54] ['▁손', '바', '닥']\n",
      "[55, 56] ['▁위', '엔']\n",
      "[57, 58, 59] ['▁기', '쁨', '의']\n",
      "[60, 61] ['▁눈', '물이']\n",
      "[62, 63] ['▁흘', '러']\n",
      "[64, 65, 66] ['▁컬', '컬', '한']\n",
      "[67, 68] ['▁목', '에']\n",
      "[69, 70] ['▁모', '주']\n",
      "[71, 72, 73] ['▁한', '잔', '을']\n",
      "[74, 75] ['▁적', '셔']\n",
      "[76] ['▁몇']\n",
      "[77] ['▁달']\n",
      "[78] ['▁포']\n",
      "[79, 80] ['▁전', '부터']\n",
      "[81, 82, 83, 84] ['▁콜', '록', '거', '리는']\n",
      "[85] ['▁아내']\n",
      "[86, 87] ['▁생각', '에']\n",
      "[88, 89, 90] ['▁그', '토', '록']\n",
      "[91, 92] ['▁먹', '고']\n",
      "[93, 94, 95] ['▁싶', '다', '던']\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기 단위로 mask하기 위해서 index 분할\n",
    "cand_idx = []  # word 단위의 index array\n",
    "for (i, token) in enumerate(tokens_org):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "        continue\n",
    "    if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\"는 단어의 시작을 의미하는 값\n",
    "        cand_idx[-1].append(i)\n",
    "    else:\n",
    "        cand_idx.append([i])\n",
    "\n",
    "# 결과확인\n",
    "for cand in cand_idx:\n",
    "    print(cand, [tokens_org[i] for i in cand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6edbf16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24, 25],\n",
       " [57, 58, 59],\n",
       " [32, 33],\n",
       " [64, 65, 66],\n",
       " [41],\n",
       " [79, 80],\n",
       " [52, 53, 54],\n",
       " [67, 68],\n",
       " [29, 30, 31],\n",
       " [91, 92],\n",
       " [23],\n",
       " [26, 27],\n",
       " [76],\n",
       " [42, 43, 44],\n",
       " [78],\n",
       " [60, 61],\n",
       " [38, 39, 40],\n",
       " [93, 94, 95],\n",
       " [9, 10, 11],\n",
       " [81, 82, 83, 84],\n",
       " [85],\n",
       " [12, 13, 14],\n",
       " [34],\n",
       " [71, 72, 73],\n",
       " [77],\n",
       " [45, 46, 47],\n",
       " [48, 49, 50],\n",
       " [28],\n",
       " [74, 75],\n",
       " [62, 63],\n",
       " [88, 89, 90],\n",
       " [5, 6],\n",
       " [35, 36, 37],\n",
       " [55, 56],\n",
       " [18, 19, 20],\n",
       " [86, 87],\n",
       " [7, 8],\n",
       " [15, 16, 17],\n",
       " [1, 2, 3, 4],\n",
       " [21, 22],\n",
       " [69, 70]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random mask를 위해서 index 순서를 섞음\n",
    "random.shuffle(cand_idx)\n",
    "cand_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "476afe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MASK]', '[MASK]', '[MASK]', '▁삼', '십', '▁전', '▁둘', '째', '번', '[MASK]', '[MASK]', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '프', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '[MASK]', '[MASK]', '[MASK]', '▁눈', '물이', '▁흘', '러', '[MASK]', '[MASK]', '[MASK]', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "mask_lms = []  # mask 된 값\n",
    "for index_set in cand_idx:\n",
    "    if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "          break\n",
    "    if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "          continue\n",
    "    dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "    for index in index_set:\n",
    "        masked_token = None\n",
    "        if dice < 0.8:  # 80% replace with [MASK]\n",
    "            masked_token = \"[MASK]\"\n",
    "        elif dice < 0.9: # 10% keep original\n",
    "            masked_token = tokens[index]\n",
    "        else:  # 10% random word\n",
    "            masked_token = random.choice(vocab_list)\n",
    "        mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "        tokens[index] = masked_token\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03223128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_idx   : [23, 24, 25, 32, 33, 41, 57, 58, 59, 64, 65, 66, 79, 80]\n",
      "mask_label : ['▁첫', '▁번', '에', '▁오', '십', '▁십', '▁기', '쁨', '의', '▁컬', '컬', '한', '▁전', '부터']\n"
     ]
    }
   ],
   "source": [
    "# 순서 정렬 및 mask_idx, mask_label 생성\n",
    "mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a813252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "    # random mask를 위해서 순서를 섞음\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0..1 사이의 확률 값\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]  # mask된 token의 index\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]  # mask된 token의 원래 값\n",
    "\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f3aec2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '[MASK]', '[MASK]', '[MASK]', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '팹', '著', '內', '[SEP]', '▁손', '바', '닥', '[MASK]', '[MASK]', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '[MASK]', '[MASK]', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '[MASK]', '[MASK]', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "mask_idx   : [21, 22, 23, 48, 49, 50, 55, 56, 62, 63, 69, 70, 86, 87]\n",
      "mask_label : ['▁많', '아', '▁첫', '▁서', '푼', '에', '▁위', '엔', '▁흘', '러', '▁모', '주', '▁생각', '에']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens, \"\\n\")\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d640db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"추적추적 비가 내리는 날이었어\n",
    "그날은 왠지 손님이 많아\n",
    "첫 번에 삼십 전 둘째 번 오십 전\n",
    "오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
    "손바닥 위엔 기쁨의 눈물이 흘러\n",
    "컬컬한 목에 모주 한잔을 적셔\n",
    "몇 달 포 전부터 콜록거리는 아내\n",
    "생각에 그토록 먹고 싶다던\n",
    "설렁탕 한 그릇을 이제는 살 수 있어\n",
    "집으로 돌아가는 길 난 문득 떠올라\n",
    "아내의 목소리가 거칠어만 가는 희박한 숨소리가\n",
    "오늘은 왠지 나가지 말라던 내 옆에 있어 달라던\n",
    "그리도 나가고 싶으면 일찍이라도 들어와 달라던\n",
    "아내의 간절한 목소리가 들려와\n",
    "나를 원망하듯 비는 점점 거세져\n",
    "싸늘히 식어가는 아내가 떠올라 걱정은 더해져\n",
    "난 몰라 오늘은 운수 좋은 날\n",
    "난 맨날 이렇게 살 수 있으면 얼마나 좋을까\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19244fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'],\n",
       " ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'],\n",
       " ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 줄 단위로 tokenizing\n",
    "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
    "doc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b56baed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이\n",
    "n_test_seq = 64\n",
    "# 최소 길이\n",
    "min_seq = 8\n",
    "# [CLS], tokens_a, [SEB], tokens_b, [SEP]\n",
    "max_seq = n_test_seq - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce078c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
      "tokens_a: 50 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에']\n",
      "tokens_b: 12 ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
      "\n",
      "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
      "tokens_a: 55 ['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라']\n",
      "tokens_b: 16 ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']\n",
      "\n",
      "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
      "tokens_a: 56 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져']\n",
      "tokens_b: 17 ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']\n",
      "\n",
      "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
      "tokens_a: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
      "tokens_b: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우, 학습 데이터를 만듭니다. \n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        #######################################\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "          \n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8fb3ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be4e7c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
      "is_next: 0\n",
      "tokens_a: 28 ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
      "tokens_b: 33 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십']\n",
      "\n",
      "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
      "is_next: 0\n",
      "tokens_a: 39 ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']\n",
      "tokens_b: 22 ['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내']\n",
      "\n",
      "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
      "is_next: 1\n",
      "tokens_a: 17 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던']\n",
      "tokens_b: 44 ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어']\n",
      "\n",
      "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
      "is_next: 1\n",
      "tokens_a: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
      "tokens_b: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "        #######################################\n",
    "        if random.random() < 0.5:  # 50% 확률로 swap\n",
    "            is_next = 0     #False\n",
    "            tokens_t = tokens_a\n",
    "            tokens_a = tokens_b\n",
    "            tokens_b = tokens_t\n",
    "        else:\n",
    "            is_next = 1    #True\n",
    "        # max_seq 보다 큰 경우 길이 조절\n",
    "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "        assert 0 < len(tokens_a)\n",
    "        assert 0 < len(tokens_b)\n",
    "\n",
    "        print(\"is_next:\", is_next)\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68f755ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
      "is_next: 0\n",
      "tokens_a: 12 ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
      "tokens_b: 49 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼']\n",
      "tokens: 64 ['[CLS]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '[SEP]']\n",
      "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 64 ['[CLS]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '[MASK]', '[MASK]', '[MASK]', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MASK]', '▁번', '에', '▁삼', '십', '가로', '[MASK]', '[MASK]', '▁번', '[MASK]', '[MASK]', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '[SEP]']\n",
      "masked index: 9 [25, 26, 27, 36, 41, 42, 43, 45, 46]\n",
      "masked label: 9 ['▁그', '날', '은', '▁첫', '▁전', '▁둘', '째', '▁오', '십']\n",
      "\n",
      "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
      "is_next: 1\n",
      "tokens_a: 12 ['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔']\n",
      "tokens_b: 49 ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거']\n",
      "tokens: 64 ['[CLS]', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '[SEP]', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거', '[SEP]']\n",
      "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 64 ['[CLS]', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '暴', '°', '[SEP]', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '[MASK]', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '[MASK]', '▁난', '▁문', '득', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁목', '소', '리가', '▁거', '[SEP]']\n",
      "masked index: 9 [11, 12, 43, 50, 54, 55, 56, 57, 58]\n",
      "masked label: 9 ['▁적', '셔', '▁살', '▁길', '▁떠', '올', '라', '▁아내', '의']\n",
      "\n",
      "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
      "is_next: 1\n",
      "tokens_a: 17 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던']\n",
      "tokens_b: 44 ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어']\n",
      "tokens: 64 ['[CLS]', '▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던', '[SEP]', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '[SEP]']\n",
      "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 64 ['[CLS]', '[MASK]', '[MASK]', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '[SEP]']\n",
      "masked index: 9 [1, 2, 15, 16, 17, 25, 26, 27, 28]\n",
      "masked label: 9 ['▁오늘', '은', '▁달', '라', '던', '▁일', '찍', '이라', '도']\n",
      "\n",
      "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
      "is_next: 0\n",
      "tokens_a: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
      "tokens_b: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
      "tokens: 25 ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]']\n",
      "segment: 25 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 25 ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]']\n",
      "masked index: 3 [7, 8, 15]\n",
      "masked label: 3 ['▁있', '으면', '▁난']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instances = []\n",
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "        if random.random() < 0.5:  # 50% 확률로 swap\n",
    "            is_next = 0    # False\n",
    "            tokens_t = tokens_a\n",
    "            tokens_a = tokens_b\n",
    "            tokens_b = tokens_t\n",
    "        else:\n",
    "            is_next = 1   # True\n",
    "        # max_seq 보다 큰 경우 길이 조절\n",
    "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "        assert 0 < len(tokens_a)\n",
    "        assert 0 < len(tokens_b)\n",
    "\n",
    "        print(\"is_next:\", is_next)\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "\n",
    "        # tokens & segment 생성\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "        segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "        print(\"tokens:\", len(tokens), tokens)\n",
    "        print(\"segment:\", len(segment), segment)\n",
    "        \n",
    "        # mask\n",
    "        tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * 0.15), vocab_list)\n",
    "        print(\"masked tokens:\", len(tokens), tokens)\n",
    "        print(\"masked index:\", len(mask_idx), mask_idx)\n",
    "        print(\"masked label:\", len(mask_label), mask_label)\n",
    "\n",
    "        instance = {\n",
    "            \"tokens\": tokens,\n",
    "            \"segment\": segment,\n",
    "            \"is_next\": is_next,\n",
    "            \"mask_idx\": mask_idx,\n",
    "            \"mask_label\": mask_label\n",
    "        }\n",
    "        instances.append(instance)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fd28c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '[MASK]', '[MASK]', '[MASK]', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MASK]', '▁번', '에', '▁삼', '십', '가로', '[MASK]', '[MASK]', '▁번', '[MASK]', '[MASK]', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [25, 26, 27, 36, 41, 42, 43, 45, 46], 'mask_label': ['▁그', '날', '은', '▁첫', '▁전', '▁둘', '째', '▁오', '십']}\n",
      "{'tokens': ['[CLS]', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '暴', '°', '[SEP]', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '[MASK]', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '[MASK]', '▁난', '▁문', '득', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁목', '소', '리가', '▁거', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [11, 12, 43, 50, 54, 55, 56, 57, 58], 'mask_label': ['▁적', '셔', '▁살', '▁길', '▁떠', '올', '라', '▁아내', '의']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 15, 16, 17, 25, 26, 27, 28], 'mask_label': ['▁오늘', '은', '▁달', '라', '던', '▁일', '찍', '이라', '도']}\n",
      "{'tokens': ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [7, 8, 15], 'mask_label': ['▁있', '으면', '▁난']}\n"
     ]
    }
   ],
   "source": [
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2aaba203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line\n",
    "        current_length += len(doc[i])\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "            # tokens & aegment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e00fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁번', '에', '▁삼', '십', '[MASK]', '▁둘', '째', 'え', '▁오', '십', '▁전', '▁오', '랜', '만에', '[MASK]', '[MASK]', '[MASK]', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [5, 8, 15, 16, 17, 36, 37, 38, 39], 'mask_label': ['▁전', '▁번', '▁받아', '보', '는', '▁눈', '물이', '▁흘', '러']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁몇', '▁달', '[MASK]', '▁전', '부터', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '[MASK]', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '[MASK]', '▁난', '▁문', '득', '▁떠', '올', '라', '[SEP]', '▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 5, 8, 9, 10, 11, 32, 39], 'mask_label': ['▁적', '셔', '▁포', '▁콜', '록', '거', '리는', '▁살', '▁길']}\n",
      "{'tokens': ['[CLS]', '에', '▁있어', '▁달', '라', '던', '▁그리', '도', '[MASK]', '[MASK]', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '[SEP]', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '썹', '숙', '촐', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '[MASK]', '[MASK]', '[MASK]', '▁더', '해', '져', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [2, 8, 9, 43, 44, 45, 57, 58, 59], 'mask_label': ['▁있어', '▁나가', '고', '▁거', '세', '져', '▁', '걱', '정은']}\n",
      "{'tokens': ['[CLS]', '▁난', '[MASK]', '[MASK]', '▁이렇게', '▁살', '[MASK]', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [2, 3, 6], 'mask_label': ['▁맨', '날', '▁수']}\n"
     ]
    }
   ],
   "source": [
    "instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "791429e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdf3a1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04dfd5199a134141956ec2395bfe237e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 lines : ['▁지', '미', '▁카', '터']\n",
      "['▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카', '터', '▁주', '니어', '(,', '▁192', '4', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주', '당', '▁출신', '▁미국', '▁3', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '▁~', '▁1981', '년', ')', '이다', '.']\n",
      "['▁그는', '▁2002', '년', '▁말', '▁인', '권', '과', '▁중', '재', '▁역할', '에', '▁대한', '▁공', '로를', '▁인정', '받아', '▁노', '벨', '▁평화', '상을', '▁받', '게', '▁되었다', '.']\n",
      "\n",
      "14 lines : ['▁수학']\n",
      "['▁수학', '(', '數', '學', ',', '▁)', '은', '▁양', ',', '▁구조', ',', '▁공간', ',', '▁변화', ',', '▁미', '적', '분', '▁등의', '▁개념', '을', '▁다루', '는', '▁학', '문', '이다', '.', '▁현대', '▁수학', '은', '▁형식', '▁논', '리를', '▁이용', '해서', '▁공', '리로', '▁구성된', '▁추', '상', '적', '▁구조를', '▁연구', '하는', '▁학', '문', '으로', '▁여겨', '지', '기도', '▁한다', '.', '▁수학', '은', '▁그', '▁구조', '와', '▁발전', '▁과정', '에서는', '▁자연', '과학', '에', '▁속하는', '▁물리', '학을', '▁비롯한', '▁다른', '▁학', '문', '들과', '▁깊', '은', '▁연', '관을', '▁맺', '고', '▁있다', '.', '▁하지만', ',', '▁어느', '▁과학', '의', '▁분야', '들과', '는', '▁달리', ',', '▁자연', '계에서', '▁관측', '되지', '▁않는', '▁개념', '들에', '▁대해서', '까지', '▁이론', '을', '▁일반', '화', '▁및', '▁추', '상', '화', '시', '킬', '▁수', '▁있다는', '▁차', '이가', '▁있다고', '▁한다', '.', '▁수', '학자', '들은', '▁그러', '한', '▁개념', '들에', '▁대해서', '▁추', '측', '을', '▁하고', ',', '▁적', '절', '하게', '▁선택', '된', '▁정의', '와', '▁공', '리', '로부터', '의', '▁엄', '밀', '한', '▁연', '역을', '▁통해', '서', '▁추', '측', '들의', '▁진', '위를', '▁파', '악', '한다', '.']\n",
      "['▁수', '학의', '▁기초', '를', '▁확', '실', '히', '▁세', '우', '기', '▁위해', ',', '▁수', '리', '논', '리', '학과', '▁집합', '론', '이', '▁발전', '하였고', ',', '▁이와', '▁더불어', '▁범', '주', '론', '이', '▁최근', '에도', '▁발전', '되고', '▁있다', '.', '▁“', '근', '본', '▁위', '기', '”', '라는', '▁말', '은', '▁대', '략', '▁19', '00', '년', '에서', '▁1930', '년', '▁사이에', '▁일어난', ',', '▁수', '학의', '▁엄', '밀', '한', '▁기초', '에', '▁대한', '▁탐', '구를', '▁상징', '적으로', '▁보여', '주는', '▁말이다', '.', '▁수', '학의', '▁엄', '밀', '한', '▁기초', '에', '▁대한', '▁몇', '▁가지', '▁의견', '▁불', '일', '치는', '▁오늘날', '에도', '▁계속', '되고', '▁있다', '.', '▁수', '학의', '▁기초', '에', '▁대한', '▁위', '기는', '▁그', '▁당시', '▁수많은', '▁논', '쟁', '에', '▁의해', '▁촉', '발', '되었으며', ',', '▁그', '▁논', '쟁', '에는', '▁칸', '토', '어의', '▁집합', '론', '과', '▁브라', '우', '어', '-', '힐', '베', '르트', '▁논', '쟁', '이', '▁포함', '되었다', '.']\n",
      "\n",
      "4 lines : ['▁수학', '▁상', '수']\n",
      "['▁수학', '에서', '▁상', '수', '란', '▁그', '▁값', '이', '▁변', '하지', '▁않는', '▁불', '변', '량', '으로', ',', '▁변', '수의', '▁반대', '말', '이다', '.', '▁물리', '▁상', '수', '와는', '▁달리', ',', '▁수학', '▁상', '수는', '▁물리', '적', '▁측정', '과는', '▁상', '관', '없이', '▁정의', '된다', '.']\n",
      "['▁특정', '▁수학', '▁상', '수', ',', '▁예를', '▁들', '면', '▁골', '롬', '-', '딕', '맨', '▁상', '수', ',', '▁프랑', '세', '즈', '-', '로', '빈', '슨', '▁상', '수', ',', '▁formula', '_1', ',', '▁레', '비', '▁상', '수', '같은', '▁상', '수는', '▁다른', '▁수학', '상', '수', '▁또는', '▁함수', '와', '▁약', '한', '▁상', '관', '관', '계', '▁또는', '▁강한', '▁상', '관', '관', '계를', '▁갖', '는다', '.']\n",
      "\n",
      "10 lines : ['▁문학']\n",
      "['▁문학', '(', '文', '學', ')', '은', '▁언', '어를', '▁예술', '적', '▁표현', '의', '▁제', '재', '로', '▁삼', '아', '▁새로운', '▁의미', '를', '▁창', '출', '하여', ',', '▁인간', '과', '▁사회', '를', '▁진', '실', '되', '게', '▁묘사', '하는', '▁예술', '의', '▁하', '위', '분', '야', '이다', '.', '▁간', '단', '하게', '▁설명', '하면', ',', '▁언', '어를', '▁통해', '▁인간의', '▁삶', '을', '▁미', '적', '(', '美', '的', ')', '으로', '▁형', '상', '화', '한', '▁것이라고', '▁볼', '▁수', '▁있다', '.', '▁문학', '은', '▁원래', '▁문', '예', '(', '文', '藝', ')', '라고', '▁부', '르는', '▁것이', '▁', '옳', '으며', ',', '▁문', '학을', '▁학', '문', '의', '▁대상', '으로서', '▁탐', '구', '하는', '▁학', '문', '의', '▁명칭', '▁역시', '▁문', '예', '학', '이다', '.', '▁문', '예', '학', '은', '▁음악', '사', '학', ',', '▁미술', '사', '학', '▁등과', '▁함께', '▁예술', '학의', '▁핵', '심', '분', '야', '로서', '▁인', '문', '학의', '▁하', '위', '범', '주에', '▁포함', '된다', '.']\n",
      "['▁반', '영', '론', '적', '▁관', '점에', '▁의한', '▁감', '상은', '▁작품', '을', '▁창', '작', '된', '▁당시', '▁시대', '▁정', '황', '과', '▁연결', '시켜', '▁감', '상', '하는', '▁입', '장', '이고', ',', '▁내', '재', '적', '▁관', '점', '의', '▁감', '상은', '▁작품', '의', '▁형식', ',', '▁내용', '에', '▁국', '한', '하여', '▁감', '상', '하는', '▁것이다', '.', '▁표현', '론', '적', '▁관', '점', '의', '▁감', '상은', '▁작가', '의', '▁전기', '적', '▁사실', '과', '▁작품', '을', '▁연결', '시켜', '▁감', '상', '하는', '▁것이', '고', ',', '▁수용', '론', '적', '▁관', '점', '의', '▁감', '상은', '▁독', '자와', '▁작품', '을', '▁연결', '시켜', '▁감', '상', '하는', '▁것을', '▁말한다', '.']\n",
      "\n",
      "10 lines : ['▁나라', '▁목록']\n",
      "['▁이', '▁문', '서는', '▁나라', '▁목록', '이며', ',', '▁전', '▁세계', '▁20', '6', '개', '▁나라', '의', '▁각', '▁현', '황', '과', '▁주', '권', '▁승', '인', '▁정보를', '▁개', '요', '▁형태로', '▁나', '열', '하고', '▁있다', '.']\n",
      "['▁위', '▁목록', '에', '▁포함', '되지', '▁않은', '▁다음', '▁국가', '는', '▁몬', '테', '비', '데', '오', '▁협', '약', '의', '▁모든', '▁조건', '을', '▁만족', '하지', '▁못', '하거나', ',', '▁자주', '적이고', '▁독립', '적', '임을', '▁주장', '하지', '▁않는', '▁국가', '이다', '.']\n",
      "\n",
      "['▁화학']\n",
      "['▁화학', '(', '化', '學', ',', '▁)', '은', '▁물질', '의', '▁성', '질', ',', '▁조성', ',', '▁구조', ',', '▁변화', '▁및', '▁그', '에', '▁수', '반', '하는', '▁에너', '지의', '▁변', '화를', '▁연구', '하는', '▁자연', '과', '학의', '▁한', '▁분야', '이다', '.', '▁물리', '학', '도', '▁역시', '▁물질', '을', '▁다루', '는', '▁학', '문', '이지만', ',', '▁물리', '학', '이', '▁원', '소', '와', '▁화', '합', '물을', '▁모두', '▁포함한', '▁물', '체의', '▁운동', '과', '▁에너', '지', ',', '▁열', '적', '·', '전', '기', '적', '·', '광', '학적', '·', '기', '계', '적', '▁속', '성을', '▁다루', '고', '▁이러한', '▁현', '상', '으로부터', '▁통일', '된', '▁이론', '을', '▁구축', '하려는', '▁것', '과는', '▁달리', '▁화학', '에서는', '▁물질', '▁자', '체를', '▁연구', '▁대상으로', '▁한다', '.', '▁화학', '은', '▁이미', '▁존재', '하는', '▁물질', '을', '▁이용하여', '▁특', '정한', '▁목', '적', '에', '▁맞', '는', '▁새로운', '▁물질', '을', '▁합', '성', '하는', '▁길', '을', '▁제공', '하며', ',', '▁이는', '▁농', '작', '물의', '▁증', '산', ',', '▁질', '병', '의', '▁치료', '▁및', '▁예', '방', ',', '▁에너', '지', '▁효', '율', '▁증', '대', ',', '▁환경', '오', '염', '▁감소', '▁등', '▁여러', '▁가지', '▁이', '점을', '▁제공', '한다', '.']\n",
      "['▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '해', '낸', '▁화', '합', '물을', '▁뜻', '하였으나', '▁지금', '은', '▁유', '기', '▁화', '합', '물의', '▁범', '위가', '▁크게', '▁넓', '어져', '▁탄', '소', '▁사', '슬', '▁또는', '▁탄', '소', '▁고', '리를', '▁가진', '▁모든', '▁화', '합', '물을', '▁뜻', '한다', '.', '▁유', '기', '화', '학의', '▁오', '랜', '▁관', '심', '사는', '▁유', '기', '▁화', '합', '물의', '▁합', '성', '▁메', '커', '니', '즘', '이다', '.', '▁현', '대에', '▁들어', '서', '▁핵', '자', '기', '▁공', '명', '법', '과', '▁X', '선', '▁결정', '학', '▁등이', '▁개발', '되어', '▁유', '기', '▁화', '합', '물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 위키가 주제별로 잘 나눠지는지 여부 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)  \n",
    "            if 0 < len(doc):\n",
    "                if 0 < count:\n",
    "                    count -= 1\n",
    "                    print(len(doc), \"lines :\", doc[0])\n",
    "                    print(doc[1])\n",
    "                    print(doc[-1])\n",
    "                    print()\n",
    "                else:\n",
    "                    break\n",
    "                doc = []\n",
    "        else:  # 빈 줄이 아니면 doc에 저장\n",
    "            pieces = vocab.encode_as_pieces(line)    \n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        print(doc[0])\n",
    "        print(doc[1])\n",
    "        print(doc[-1])\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21bda10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283f009ecd454c088e519c64159e16c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 21 instances: 10\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '[MASK]', '[MASK]', '[MASK]', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [10, 11, 12, 13, 14, 15, 41, 42, 43], 'mask_label': ['▁합', '성', '섬', '유', '등', '의', '▁화', '합', '물을']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [23, 24, 25, 26, 27, 53, 54, 55, 56], 'mask_label': ['▁유', '기', '화', '학', '에서', '▁화', '합', '물', '은']}\n",
      "\n",
      "doc: 14 instances: 7\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '[MASK]', '[MASK]', '[MASK]', '▁화', '합', '물을', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [38, 39, 40, 44, 45, 46, 47, 48, 49], 'mask_label': ['▁탄', '소로', '▁이루어진', '▁연구', '하는', '▁분', '과', '이다', '.']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '[MASK]', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '[MASK]', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 40, 44, 45, 50, 51, 52, 61, 62], 'mask_label': ['으로', '▁이루어진', '▁연구', '하는', '▁원래', '▁유', '기', '▁추', '출']}\n",
      "\n",
      "doc: 4 instances: 2\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [16, 17, 18, 19, 20, 44, 45, 51, 52], 'mask_label': ['▁고', '분', '자', '물', '질', '▁연구', '하는', '▁유', '기']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁고', '분', '자', '물', '질', '[MASK]', '[MASK]', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 10, 11, 12, 13, 14, 15, 21, 22], 'mask_label': ['으로', '▁합', '성', '섬', '유', '등', '의', '▁등', '도']}\n",
      "\n",
      "doc: 10 instances: 5\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '許', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '적', '惠', '▁동물', '로부터', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [40, 53, 54, 55, 56, 57, 58, 61, 62], 'mask_label': ['▁이루어진', '▁화', '합', '물', '은', '▁식물', '이나', '▁추', '출']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '[MASK]', '[MASK]', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [23, 24, 25, 26, 27, 38, 39, 51, 52], 'mask_label': ['▁유', '기', '화', '학', '에서', '▁탄', '소로', '▁유', '기']}\n",
      "\n",
      "doc: 10 instances: 5\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '[MASK]', '[MASK]', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 46, 47, 48, 49, 51, 52, 57, 58], 'mask_label': ['으로', '▁분', '과', '이다', '.', '▁유', '기', '▁식물', '이나']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [44, 45, 50, 53, 54, 55, 56, 61, 62], 'mask_label': ['▁연구', '하는', '▁원래', '▁화', '합', '물', '은', '▁추', '출']}\n",
      "\n",
      "doc: 31 instances: 15\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '[MASK]', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 23, 24, 25, 26, 27, 40, 51, 52], 'mask_label': ['으로', '▁유', '기', '화', '학', '에서', '▁이루어진', '▁유', '기']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '[MASK]', '[MASK]', '[MASK]', '▁연구', '하는', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [23, 24, 25, 26, 27, 41, 42, 43, 50], 'mask_label': ['▁유', '기', '화', '학', '에서', '▁화', '합', '물을', '▁원래']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instance 생성 기능 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "            if 0 < len(doc):\n",
    "                instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "                # save\n",
    "                print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "                print(instances[0])\n",
    "                print(instances[-1])\n",
    "                print()\n",
    "                doc = []\n",
    "                if 0 < count:  # 테스트를 위해서 부분 처리함\n",
    "                    count -= 1\n",
    "                else:\n",
    "                    break\n",
    "        else:  # doc에 저장\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        instances = create_pretrain_instances(doc, 128)\n",
    "        # save\n",
    "        print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "        print(instances[0])\n",
    "        print(instances[-1])\n",
    "        print()\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49651bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):        # 생성되는 단어 목록이 unknown인 경우는 제거합니다. \n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "                    if 0 < len(doc):\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)\n",
    "                doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2033d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb98723fe9ed45b187a1fc29c838bdd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrain_json_path = os.getenv('HOME')+'/aiffel/bert_pretrain/data/bert_pre_train.json'\n",
    "\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dafd22fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918189"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa36c81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 0,\n",
       " 0,\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 128\n",
    "max_seq = n_seq - 3\n",
    "\n",
    "enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "\n",
    "enc_tokens[0], enc_tokens[-1], segments[0], segments[-1], labels_nsp[0], labels_nsp[-1], labels_mlm[0], labels_mlm[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fc01bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f5d47f9c4646d1931b9a1ecee37c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/918189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '일', '▁~', '[MASK]', '[MASK]', '▁민주', '당', '▁출신', '▁미국', '▁3', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '[MASK]', '▁1981', '년', ')', '이다', '.', '▁지', '미', '▁카', '터', '는', '[MASK]', '[MASK]', '[MASK]', '▁섬', '터', '▁카운', '티', '▁플', '레', '인', '스', '▁마을', '에서', '▁태어났다', '.', '▁조지', '아', '▁공', '과', '대학교', '를', '▁졸업', '하였다', '.', '▁그', '▁후', '▁해', '군에', '▁들어가', '▁전', '함', '·', '원', '자', '력', '·', '잠', '수', '함', '의', '▁승', '무', '원으로', '▁일', '하였다', '.', '▁195', '3', '년', '▁미국', '▁해군', '▁대', '위로', '▁예', '편', '하였고', '▁이후', '▁땅', '콩', '·', '면', '화', '[MASK]', '▁가', '꿔', '▁많은', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁그의', '▁별', '명이', '▁\"', '땅', '콩', '▁농', '부', '\"', '▁(', 'P', 'e', 'an', 'ut', '▁F', 'ar', 'm', 'er', ')', '로', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁지', '미', '▁카', '터', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [3, 4, 9, 10, 11, 17, 28, 29, 30, 90, 94, 95, 96, 97, 98, 119, 120, 121], 'mask_label': ['▁)', '는', '▁3', '9', '번째', '▁~', '▁조지', '아', '주', '▁등을', '▁돈', '을', '▁벌', '었다', '.', '▁알려', '졌다', '.']}\n",
      "enc_token: [5, 3629, 203, 6, 6, 1114, 3724, 788, 243, 49, 3632, 796, 663, 1647, 3682, 3682, 3625, 6, 3008, 3625, 3616, 16, 3599, 18, 3686, 207, 3714, 3602, 6, 6, 6, 630, 3714, 3565, 3835, 429, 3740, 3628, 3626, 1369, 10, 1605, 3599, 1755, 3630, 41, 3644, 830, 3624, 1135, 52, 3599, 13, 81, 87, 1501, 2247, 25, 3779, 3873, 3667, 3631, 3813, 3873, 4196, 3636, 3779, 3601, 249, 3725, 1232, 33, 52, 3599, 479, 3652, 3625, 243, 2780, 14, 1509, 168, 3877, 414, 165, 1697, 4290, 3873, 3703, 3683, 6, 21, 5007, 399, 6, 6, 6, 6, 6, 307, 587, 931, 103, 4313, 4290, 613, 3638, 3718, 98, 3878, 3656, 256, 2543, 309, 337, 3735, 181, 3616, 3603, 6, 6, 6, 4, 18, 3686, 207, 3714, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0  241 3602    0    0    0    0   49 3632  796    0    0\n",
      "    0    0    0  203    0    0    0    0    0    0    0    0    0    0\n",
      " 1755 3630 3646    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0  593    0    0    0 1927 3607  813   17\n",
      " 3599    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0  489  376 3599    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '아', '▁주', '▁상', '원', '▁의원', '▁선거', '에서', '▁낙', '선', '하나', '▁그', '▁선거', '가', '▁부정', '선거', '▁', '였', '음을', '▁입', '증', '하게', '▁되어', '▁당선', '되고', ',', '▁196', '6', '년', '▁조지', '아', '▁주', '▁지', '사', '▁선거', '에', '▁낙', '선', '하지만', '[MASK]', '[MASK]', '▁조지', '아', '▁주', '▁지', '사를', '▁역임', '했다', '.', '▁대통령', '이', '▁되', '기', '▁전', '▁조지', '아', '주', '總', '▁가르', '▁파', '▁두', '번', '▁연', '임', '했으며', ',', '[MASK]', '[MASK]', '▁1975', '년까지', '▁조지', '아', '▁지', '사로', '[MASK]', '[MASK]', '[MASK]', '▁조지', '아', '▁주', '지', '사로', '▁지', '내', '면서', ',', '▁미국', '에', '[MASK]', '▁흑', '인', '▁등', '용', '법을', '▁내', '세', '웠다', '.', '[SEP]', '▁1976', '년', '▁대통령', '▁선거', '에', '▁민주', '당', '▁후보', '로', '▁출', '마', '하여', '▁도', '덕', '주의', '[MASK]', '[MASK]', '▁내', '세', '워', ',', '[MASK]', '[MASK]', '▁누', '르고', '지', '▁달러', '잴', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [39, 40, 57, 58, 59, 66, 67, 74, 75, 76, 88, 114, 115, 120, 121, 124, 125, 126], 'mask_label': ['▁1970', '년', '▁상', '원의', '원을', '▁1971', '년부터', '▁근무', '했다', '.', '▁사는', '▁정책', '으로', '▁포', '드를', '▁당선', '되었다', '.']}\n",
      "enc_token: [5, 3630, 37, 76, 3667, 2378, 822, 10, 1567, 3668, 3294, 13, 822, 3608, 2386, 2163, 3596, 3671, 969, 213, 3929, 173, 607, 2387, 317, 3604, 386, 3673, 3625, 1755, 3630, 37, 18, 3620, 822, 3600, 1567, 3668, 1447, 6, 6, 1755, 3630, 37, 18, 451, 1398, 31, 3599, 663, 3597, 450, 3614, 25, 1755, 3630, 3646, 5400, 2190, 146, 157, 3821, 61, 3773, 530, 3604, 6, 6, 3409, 673, 1755, 3630, 18, 982, 6, 6, 6, 1755, 3630, 37, 3610, 982, 18, 3754, 151, 3604, 243, 3600, 6, 1733, 3628, 50, 3717, 2046, 114, 3692, 1853, 3599, 4, 3306, 3625, 663, 822, 3600, 1114, 3724, 958, 3603, 117, 3674, 54, 75, 4089, 238, 6, 6, 114, 3692, 3964, 3604, 6, 6, 807, 2056, 3610, 2178, 7877, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0 1921 3625    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0   76  955  928    0    0    0    0    0    0 3372  523    0    0\n",
      "    0    0    0    0 2711   31 3599    0    0    0    0    0    0    0\n",
      "    0    0    0    0 3554    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0 1421    9    0    0    0    0  119 1486    0    0 2387   43\n",
      " 3599    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁개발', '을', '▁촉', '구', '했으나', '▁공', '화', '당의', '[MASK]', '[MASK]', '▁무', '산', '되었다', '.', '▁카', '터', '는', '[MASK]', '[MASK]', '[MASK]', '▁이스라엘', '을', '[MASK]', '[MASK]', '[MASK]', '▁캠', '프', '▁데이', '비', '드에서', '▁안', '와', '르', '▁사', '다', '트', '[MASK]', '[MASK]', '▁메', '나', '헴', '▁베', '긴', '▁수상', '과', '▁함께', '▁중', '동', '▁평', '화를', '▁맡았다', '▁캠', '프', '데', '이', '비', '드', '▁협', '정을', '▁체결', '했다', '.', '[SEP]', '▁그러나', '[MASK]', '▁공', '화', '당', '과', '[MASK]', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.', '▁카', '터', '는', '▁1970', '년대', '[MASK]', '[MASK]', '▁대한민국', '▁등', '▁인', '권', '▁후', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 10, 11, 19, 20, 21, 24, 25, 26, 38, 39, 52, 66, 71, 100, 101, 120, 121], 'mask_label': ['지', '▁반', '대로', '▁이집', '트', '와', '▁조정', '하여', ',', '▁대통령', '과', '▁위한', '▁이것은', '▁미국의', '▁소련', '과', '▁후반', '▁당시']}\n",
      "enc_token: [5, 6, 570, 3607, 2270, 3653, 1003, 41, 3683, 1547, 6, 6, 107, 3726, 43, 3599, 207, 3714, 3602, 6, 6, 6, 3426, 3607, 6, 6, 6, 2432, 3721, 965, 3694, 3552, 172, 3665, 3699, 15, 3598, 3677, 6, 6, 334, 3637, 5887, 271, 4099, 1011, 3644, 280, 35, 3658, 232, 934, 1896, 2432, 3721, 3736, 3597, 3694, 3681, 617, 666, 2525, 31, 3599, 4, 330, 6, 41, 3683, 3724, 3644, 6, 2670, 3628, 164, 1314, 141, 3720, 3607, 1213, 4174, 3598, 3599, 2995, 3625, 456, 3928, 3708, 10, 230, 3643, 2714, 2793, 3676, 3827, 9, 1435, 2521, 3599, 276, 1302, 3644, 30, 3619, 3751, 2835, 107, 3614, 1956, 617, 1824, 53, 3628, 31, 3599, 207, 3714, 3602, 1921, 596, 6, 6, 410, 50, 42, 3830, 81, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0 3610    0    0    0    0    0    0    0    0  141  448    0    0\n",
      "    0    0    0    0    0 2703 3677 3665    0    0 3358   54 3604    0\n",
      "    0    0    0    0    0    0    0    0    0    0  663 3644    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0  521    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0 1487    0    0    0\n",
      "    0  679    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0 1302 3644    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0 1840  316    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁부', '딪', '혀', '吾', '팡', '▁계열', '도에', '▁완', '전', '철', '수', '▁대신', '▁6', ',000', '명을', '▁감', '축', '하는', '▁데', '▁그', '쳤다', '.', '▁또한', '▁박', '정', '희', '[MASK]', '[MASK]', '▁인', '권', '▁문제', '▁등', '과의', '▁논란', '으로', '▁불', '협', '화', '음을', '▁', '냈', '으나', ',', '▁1979', '년', '▁6', '월', '▁하', '순', ',', '▁대한민국', '을', '▁방문', '하여', '▁관계', '가', '▁다', '소', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁그러나', '▁주', '[MASK]', '▁미국', '▁대사', '관', '▁인', '질', '▁사건', '에서', '▁인', '질', '▁구', '출', '▁실패', '를', '[MASK]', '▁1980', '년', '[MASK]', '[MASK]', '[MASK]', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '▁', '져', '[MASK]', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '[MASK]', '▁1980', '년', '▁하계', '▁올림픽', '에', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 2, 6, 7, 8, 9, 29, 30, 61, 62, 63, 67, 81, 84, 85, 86, 99, 121], 'mask_label': ['▁반', '대에', '▁주', '한', '미', '군은', '▁정', '권의', '▁회복', '되었다', '.', '▁이란', '▁이유로', '▁대통령', '▁선거', '에서', '▁결국', '▁인해']}\n",
      "enc_token: [5, 6, 6, 51, 5148, 4178, 6398, 4758, 2440, 1464, 443, 3640, 3917, 3636, 1083, 125, 847, 859, 209, 3909, 38, 189, 13, 1523, 3599, 276, 338, 3642, 4055, 6, 6, 42, 3830, 550, 50, 786, 2408, 9, 128, 3993, 3683, 969, 3596, 4121, 191, 3604, 2995, 3625, 125, 3662, 27, 3946, 3604, 410, 3607, 2017, 54, 704, 3608, 29, 3688, 6, 6, 6, 4, 330, 37, 6, 243, 2630, 3708, 42, 3892, 636, 10, 42, 3892, 73, 3771, 1579, 3624, 6, 1640, 3625, 6, 6, 6, 41, 3683, 1547, 194, 4044, 3681, 1169, 3803, 958, 113, 3596, 3944, 6, 174, 2087, 1579, 31, 3599, 276, 273, 3614, 150, 329, 870, 3713, 1302, 3601, 26, 2986, 3733, 1323, 3232, 636, 9, 6, 1640, 3625, 2219, 779, 3600, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0  141  867    0    0    0   37 3612 3686  941    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0   36 2649    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0 3332   43 3599    0    0    0 3290    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0 1827    0    0\n",
      "  663  822   10    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0  875    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0  751    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '한', '▁뒤', '▁민주', '주의', '▁실', '현', '을', '▁위해', '▁제', '▁3', '세', '계의', '[MASK]', '▁감', '시', '▁활동', '▁및', '▁기', '니', '[MASK]', '[MASK]', '[MASK]', '▁의한', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁질', '병', '▁방', '재', '를', '▁위해', '▁힘', '썼', '다', '.', '▁미국의', '▁빈', '곤', '층', '▁지원', '▁활동', ',', '▁사랑', '의', '[MASK]', '[MASK]', '[MASK]', '▁운동', ',', '▁국제', '▁분', '쟁', '▁중', '재', '▁등의', '[MASK]', '[MASK]', '▁했다', '.', '[SEP]', '[MASK]', '[MASK]', '▁~', '▁1980', '년', '▁대한민국의', '▁정치적', '▁격', '변', '기', '[MASK]', '▁대통령', '이었던', '▁그는', '▁이에', '▁대해', '▁애', '매', '한', '▁태', '도를', '▁보', '였고', ',', '▁이는', '▁후에', '▁대한민국', '▁내에서', '▁고', '조', '되는', '▁반', '미', '▁운동', '의', '▁한', '▁원', '인이', '▁', '됐다', '.', '▁10', '월', '▁26', '일', ',', '▁박', '정', '희', '▁대통령', '이', '▁김', '재', '규', '▁중앙', '정보', '부', '장에', '▁의해', '▁살해', '된', '▁것에', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [13, 20, 21, 22, 24, 25, 26, 27, 28, 29, 49, 50, 51, 60, 61, 65, 66, 75], 'mask_label': ['▁선거', '▁벌', '레', '에', '▁드', '라', '쿤', '쿠', '르', '스', '▁집', '짓', '기', '▁활동', '도', '▁1979', '년', '▁당시의']}\n",
      "enc_token: [5, 3612, 339, 1114, 238, 158, 3756, 3607, 231, 30, 49, 3692, 1654, 6, 209, 3623, 375, 228, 24, 3733, 6, 6, 6, 1332, 6, 6, 6, 6, 6, 6, 761, 3886, 95, 3729, 3624, 231, 947, 4437, 3598, 3599, 679, 1412, 4234, 4083, 770, 375, 3604, 1424, 3601, 6, 6, 6, 887, 3604, 605, 147, 3972, 35, 3729, 507, 6, 6, 345, 3599, 4, 6, 6, 203, 1640, 3625, 447, 2843, 1032, 3889, 3614, 6, 663, 1277, 202, 695, 433, 442, 3823, 3612, 227, 701, 47, 2470, 3604, 594, 1140, 410, 3428, 70, 3676, 267, 141, 3686, 887, 3601, 34, 129, 828, 3596, 1027, 3599, 131, 3662, 981, 3629, 3604, 338, 3642, 4055, 663, 3597, 200, 3729, 3958, 782, 2275, 3638, 1312, 355, 2591, 3711, 2057, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0  822\n",
      "    0    0    0    0    0    0  813 3740 3600    0  311 3635 4956 3937\n",
      " 3699 3626    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0  313 4333 3614    0    0    0    0\n",
      "    0    0    0    0  375 3627    0    0    0 2995 3625    0    0    0\n",
      "    0    0    0    0    0 3195    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁미국', '이', '▁북', '핵', '▁위', '기', ',', '▁코', '소', '보', '▁전쟁', ',', '▁이', '라크', '▁전쟁', '과', '▁같이', '▁미국', '이', '▁군사', '적', '▁행', '동을', '[MASK]', '[MASK]', '[MASK]', '▁선택', '하는', '[MASK]', '[MASK]', '▁사고', '를', '▁버', '리고', '▁군사', '적', '▁행', '동을', '[MASK]', '[MASK]', '[MASK]', '▁행', '위에', '[MASK]', '▁깊', '은', '▁유', '감을', '▁표시', '▁하며', '▁미국의', '▁군사', '적', '[MASK]', '[MASK]', '▁강한', '▁반대', '[MASK]', '[MASK]', '▁보', '이고', '▁있다', '.', '[SEP]', '▁특히', '▁국제', '▁분', '쟁', '▁조', '정을', '▁위해', '▁북한', '의', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁아이', '티', '의', '▁세', '드', '라스', '▁장', '군', ',', '▁팔', '레', '인', '스타', '인의', '▁하', '마', '스', ',', '▁보', '스', '니아', '의', '▁세르', '비아', '계', '▁정', '권', '▁같이', '[MASK]', '▁정부', '에', '▁대해', '▁협', '상을', '▁거부', '하면서', '▁사', '태', '의', '▁위', '기를', '▁초', '래', '한', '▁인물', '▁및', '▁단', '체를', '▁직접', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [24, 25, 26, 29, 30, 39, 40, 41, 44, 54, 55, 58, 59, 74, 75, 76, 77, 106], 'mask_label': ['▁최', '후', '로', '▁전통', '적', '▁선', '행', '하는', '▁대해', '▁활동', '에', '▁입', '장을', '▁김', '일', '성', ',', '▁미국']}\n",
      "enc_token: [5, 243, 3597, 251, 4166, 45, 3614, 3604, 258, 3688, 3672, 506, 3604, 8, 3553, 506, 3644, 733, 243, 3597, 1250, 3657, 236, 1629, 6, 6, 6, 1715, 38, 6, 6, 1646, 3624, 407, 999, 1250, 3657, 236, 1629, 6, 6, 6, 236, 1157, 6, 1910, 3613, 46, 2196, 2466, 1368, 679, 1250, 3657, 6, 6, 2632, 1216, 6, 6, 47, 458, 28, 3599, 4, 698, 605, 147, 3972, 53, 666, 231, 1876, 3601, 6, 6, 6, 6, 520, 3835, 3601, 74, 3681, 1951, 104, 3722, 3604, 961, 3740, 3628, 936, 692, 27, 3674, 3626, 3604, 47, 3626, 491, 3601, 3189, 852, 3704, 36, 3830, 733, 6, 513, 3600, 433, 617, 460, 2324, 421, 15, 3800, 3601, 45, 333, 192, 3808, 3612, 1178, 228, 164, 1396, 1069, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0  130 3706 3603    0\n",
      "    0 1306 3657    0    0    0    0    0    0    0    0   57 3752   38\n",
      "    0    0  433    0    0    0    0    0    0    0    0    0  375 3600\n",
      "    0    0  213  480    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0  200 3629 3650 3604    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0  243    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for i, line in enumerate(tqdm(f, total=total)):\n",
    "        if 5 < i:  # 테스트를 위해서 5개만 확인\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # encoder token\n",
    "        enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "        enc_token += [0] * (n_seq - len(enc_token))\n",
    "        # segment\n",
    "        segment = data[\"segment\"]\n",
    "        segment += [0] * (n_seq - len(segment))\n",
    "        # nsp label\n",
    "        label_nsp = data[\"is_next\"]\n",
    "        # mlm label\n",
    "        mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "        mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "        label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "        label_mlm[mask_idx] = mask_label\n",
    "\n",
    "        print(data)\n",
    "        print(\"enc_token:\", enc_token)\n",
    "        print(\"segment:\", segment)\n",
    "        print(\"label_nsp:\", label_nsp)\n",
    "        print(\"label_mlm:\", label_mlm)\n",
    "        print()\n",
    "\n",
    "        assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "        enc_tokens[i] = enc_token\n",
    "        segments[i] = segment\n",
    "        labels_nsp[i] = label_nsp\n",
    "        labels_mlm[i] = label_mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99d35632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "458fbe26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e33d67334e4c6f9a0ba11b5b86b7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    }
   ],
   "source": [
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98a3cdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([   5, 3629,  203,    6,    6, 1114, 3724,  788,  243,   49, 3632,\n",
       "          796,  663, 1647, 3682, 3682, 3625,    6, 3008, 3625, 3616,   16,\n",
       "         3599,   18, 3686,  207, 3714, 3602,    6,    6,    6,  630, 3714,\n",
       "         3565, 3835,  429, 3740, 3628, 3626, 1369,   10, 1605, 3599, 1755,\n",
       "         3630,   41, 3644,  830, 3624, 1135,   52, 3599,   13,   81,   87,\n",
       "         1501, 2247,   25, 3779, 3873, 3667, 3631, 3813, 3873, 4196, 3636,\n",
       "         3779, 3601,  249, 3725, 1232,   33,   52, 3599,  479, 3652, 3625,\n",
       "          243, 2780,   14, 1509,  168, 3877,  414,  165, 1697, 4290, 3873,\n",
       "         3703, 3683,    6,   21, 5007,  399,    6,    6,    6,    6,    6,\n",
       "          307,  587,  931,  103, 4313, 4290,  613, 3638, 3718,   98, 3878,\n",
       "         3656,  256, 2543,  309,  337, 3735,  181, 3616, 3603,    6,    6,\n",
       "            6,    4,   18, 3686,  207, 3714,    4], dtype=int32),\n",
       " memmap([   5, 3676,  848, 3784, 1931,   58, 3676,  416, 2316, 3619, 3625,\n",
       "         3617, 3744, 4335,   12, 3625, 3616,  175, 3662,    7, 3629,  203,\n",
       "            6,    6,    6,    6,    6,    6,  143, 3625, 3616,  131, 3662,\n",
       "          342, 3629, 3616, 3602,  176,  334,  829, 1115, 3665,    6,    6,\n",
       "         3451, 1633,  375,  671, 1644, 3608,  547, 3423,  765,  815, 3604,\n",
       "            6,    6,    6, 2375, 3608, 3604,  532, 2589, 3599,    4,  307,\n",
       "          323,    6,  321, 3611,  622,  122, 3725, 3620, 3627, 3837, 3608,\n",
       "            6,  176,  268, 4082,   94,  567, 4014, 3617, 7474, 3616, 3830,\n",
       "           66, 3590,  307,  192, 1272,  158, 3788,  353, 3599,  202,  316,\n",
       "         3600,  176,   10,  323,  476, 3663, 1329,  605,  238, 3631, 2470,\n",
       "         3604, 1939,  106, 3627,   13,    6,    6, 1128,   48,    6,    6,\n",
       "          848, 3784, 3833,    8, 3637, 2263,    4], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 0,\n",
       " 1,\n",
       " memmap([   0,    0,    0,  241, 3602,    0,    0,    0,    0,   49, 3632,\n",
       "          796,    0,    0,    0,    0,    0,  203,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0, 1755, 3630, 3646,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,  593,    0,    0,    0, 1927, 3607,  813,   17, 3599,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,  489,  376,\n",
       "         3599,    0,    0,    0,    0,    0,    0], dtype=int32),\n",
       " memmap([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          578, 3652, 3625, 3617, 4148, 3665,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0, 1381, 4148,\n",
       "         3451,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          752, 3608, 3604,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0, 2143,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          347,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,  162,  490,    0,    0,   28, 3599,\n",
       "            0,    0,    0,    0,    0,    0,    0], dtype=int32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84c20a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520a9a6a",
   "metadata": {},
   "source": [
    "## 2. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0a3770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37c91a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25b01271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6967f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a499ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: position embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08b7fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "604207b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b294380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, \n",
    "                                         kernel_initializer=kernel_initializer(), \n",
    "                                         bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, \n",
    "                                         kernel_initializer=kernel_initializer(), \n",
    "                                         bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75961591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "98645ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96861f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df8afd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd3a2df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 8007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \n",
    "                 \"dropout\": 0.1, \"d_ff\": 1024, \n",
    "                 \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \n",
    "                 \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb4e339c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2/2 [==============================] - 26s 20ms/step - loss: 9.7747 - nsp_loss: 0.7198 - mlm_loss: 9.0549 - nsp_acc: 0.6000 - mlm_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 8.5973 - nsp_loss: 0.6165 - mlm_loss: 7.9808 - nsp_acc: 0.8000 - mlm_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f64752b7a00>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 10\n",
    "\n",
    "# test inputs\n",
    "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "segments = np.random.randint(0, 2, (10, n_seq))\n",
    "labels_nsp = np.random.randint(0, 2, (10,))\n",
    "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "\n",
    "test_model = build_model_pre_train(config)\n",
    "test_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, \n",
    "                   optimizer=tf.keras.optimizers.Adam(), metrics=[\"acc\"])\n",
    "\n",
    "# test model fit\n",
    "test_model.fit((enc_tokens, segments), (labels_nsp, labels_mlm), epochs=2, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e55fb24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2931bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b12819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "534f0b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Step')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "# Visualization\n",
    "plt.plot(lrs, 'g-', label='learning_rate')\n",
    "plt.xlabel('Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "19160ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 256), (None, 4485632     enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            66304       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 8007)   0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 4,551,936\n",
      "Trainable params: 4,551,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e7456c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 20000\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, \n",
    "                               warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), \n",
    "                        optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "80f84e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 246s 122ms/step - loss: 19.5794 - nsp_loss: 0.6502 - mlm_loss: 18.9292 - nsp_acc: 0.5889 - mlm_lm_acc: 0.1086\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.10859, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 243s 122ms/step - loss: 17.5017 - nsp_loss: 0.6226 - mlm_loss: 16.8791 - nsp_acc: 0.6190 - mlm_lm_acc: 0.1301\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.10859 to 0.13011, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 243s 122ms/step - loss: 15.9251 - nsp_loss: 0.6162 - mlm_loss: 15.3089 - nsp_acc: 0.6246 - mlm_lm_acc: 0.1502\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.13011 to 0.15017, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 243s 122ms/step - loss: 14.1253 - nsp_loss: 0.6122 - mlm_loss: 13.5131 - nsp_acc: 0.6297 - mlm_lm_acc: 0.1896\n",
      "\n",
      "Epoch 00004: mlm_lm_acc improved from 0.15017 to 0.18957, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 243s 122ms/step - loss: 13.3615 - nsp_loss: 0.6074 - mlm_loss: 12.7541 - nsp_acc: 0.6388 - mlm_lm_acc: 0.2127\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.18957 to 0.21271, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 243s 122ms/step - loss: 12.9158 - nsp_loss: 0.6023 - mlm_loss: 12.3134 - nsp_acc: 0.6458 - mlm_lm_acc: 0.2273\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.21271 to 0.22727, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 243s 122ms/step - loss: 12.6095 - nsp_loss: 0.5968 - mlm_loss: 12.0127 - nsp_acc: 0.6587 - mlm_lm_acc: 0.2377\n",
      "\n",
      "Epoch 00007: mlm_lm_acc improved from 0.22727 to 0.23772, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 243s 122ms/step - loss: 12.4039 - nsp_loss: 0.5912 - mlm_loss: 11.8127 - nsp_acc: 0.6684 - mlm_lm_acc: 0.2448\n",
      "\n",
      "Epoch 00008: mlm_lm_acc improved from 0.23772 to 0.24479, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 243s 122ms/step - loss: 12.2780 - nsp_loss: 0.5869 - mlm_loss: 11.6911 - nsp_acc: 0.6756 - mlm_lm_acc: 0.2489\n",
      "\n",
      "Epoch 00009: mlm_lm_acc improved from 0.24479 to 0.24891, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 243s 122ms/step - loss: 12.2203 - nsp_loss: 0.5842 - mlm_loss: 11.6361 - nsp_acc: 0.6819 - mlm_lm_acc: 0.2504\n",
      "\n",
      "Epoch 00010: mlm_lm_acc improved from 0.24891 to 0.25040, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n"
     ]
    }
   ],
   "source": [
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", \n",
    "                                                  monitor=\"mlm_lm_acc\", verbose=1, \n",
    "                                                  save_best_only=True, \n",
    "                                                  mode=\"max\", save_freq=\"epoch\", \n",
    "                                                  save_weights_only=True)\n",
    "# Model Training\n",
    "history = pre_train_model.fit(pre_train_inputs, pre_train_labels, \n",
    "                              epochs=epochs, batch_size=batch_size, \n",
    "                              callbacks=[save_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d4c5656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'g--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'r-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ec748c",
   "metadata": {},
   "source": [
    "## 3. Project Retrospective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71fcc95",
   "metadata": {},
   "source": [
    "+ 말 그대로 Pretrained된 모델을 그대로 가져와 쓰는 것이라, NLP를 더 쉽게, 많이 쓸 수 있게 하려는 개발자와 연구자들의 노력을 조금씩 느낄 수 있었다. 그만큼 모델 내에 날 것으로 커스텀할 수 있는 영역은 점점 좁아지는 것이므로, BERT 이전에 (상대적으로) 비효율적일 수 있는 모델로 구현을 가르친 의도도 이해할 수 있었다.\n",
    "+ 마지막인 HuggingFace Framework 노드를 보니, 프로젝트에 적용시켜서 하면 훨씬 빠르게 결과물을 낼 수 있겠다는 생각이 들었다. 이 부분을 참고하여 프로젝트 계획서 설계를 마무리할 계획이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
