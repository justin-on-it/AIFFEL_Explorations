{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[GD-07] BERT Pretrained model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [GD-07] BERT Pretrained Model\n",
        "\"Going Deeper Node 14. Making BERT Pretrained model\" / 2022. 04. 21 (Mon) 이형주\n",
        "\n",
        "## Contents\n",
        "---\n",
        "- **1. Environment Setup**\n",
        "- **2. Modeling**\n",
        "- **3. Project Retrospective**\n",
        "\n",
        "## Rubric 평가기준\n",
        "---\n",
        "\n",
        "|  평가문항  |  상세기준  |\n",
        "|:---------|:---------|\n",
        "|1. 한글 코퍼스를 가공하여 BERT pretrain용 데이터셋을 잘 생성하였다.|MLM, NSP task의 특징이 잘 반영된 pretrain용 데이터셋 생성과정이 체계적으로 진행되었다.\n",
        "|2. 구현한 BERT 모델의 학습이 안정적으로 진행됨을 확인하였다.|학습진행 과정 중에 MLM, NSP loss의 안정적인 감소가 확인되었다.\n",
        "3. 1M짜리 mini BERT 모델의 제작과 학습이 정상적으로 진행되었다.|학습된 모델 및 학습과정의 시각화 내역이 제출되었다."
      ],
      "metadata": {
        "id": "cBEMKk1viKoc"
      }
    }
  ]
}