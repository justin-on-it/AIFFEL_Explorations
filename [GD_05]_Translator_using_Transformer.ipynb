{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332b8927",
   "metadata": {},
   "source": [
    "# [GD-10] Transformer - Translator\n",
    "\"Going Deeper Node 10. Making translator using Transformer\" / 2022. 04. 12 (Tue) 이형주\n",
    "\n",
    "## Contents\n",
    "---\n",
    "- **1. Environment Setup**\n",
    "- **2. Modeling**\n",
    "- **3. Project Retrospective**\n",
    "\n",
    "## Rubric 평가기준\n",
    "---\n",
    "\n",
    "|  평가문항  |  상세기준  |\n",
    "|:---------|:---------|\n",
    "|1. 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 잘 이루어졌다.|데이터 정제, SentencePiece를 활용한 토큰화 및 데이터셋 구축의 과정이 지시대로 진행되었다.\n",
    "|2. Transformer 번역기 모델이 정상적으로 구동된다.|Transformer 모델의 학습과 추론 과정이 정상적으로 진행되어, 한-영 번역기능이 정상 동작한다.\n",
    "|3. 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.|제시된 문장에 대한 그럴듯한 영어 번역문이 생성되며, 시각화된 Attention Map으로 결과를 뒷받침한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6249348",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645e21d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "fonts-nanum is already the newest version (20180306-3).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  accountsservice-ubuntu-schemas bc bluez-obexd cups cups-browsed cups-client\n",
      "  cups-common cups-core-drivers cups-daemon cups-filters\n",
      "  cups-filters-core-drivers cups-ipp-utils cups-ppdc cups-server-common\n",
      "  fonts-droid-fallback fonts-noto-mono fonts-urw-base35 ghostscript\n",
      "  gir1.2-dbusmenu-glib-0.4 gnome-bluetooth gnome-power-manager\n",
      "  gnome-screensaver gsettings-ubuntu-schemas gvfs-backends indicator-applet\n",
      "  indicator-application indicator-appmenu indicator-bluetooth indicator-common\n",
      "  indicator-datetime indicator-keyboard indicator-messages indicator-power\n",
      "  indicator-printers indicator-session indicator-sound jayatana\n",
      "  libaccounts-glib0 libbamf3-2 libcdio-cdda2 libcdio-paranoia2 libcdio18\n",
      "  libcupsfilters1 libfcitx-config4 libfcitx-gclient1 libfcitx-utils0\n",
      "  libfontembed1 libgnome-panel0 libgs9 libgs9-common libido3-0.1-0 libijs-0.35\n",
      "  libindicator3-7 libjbig2dec0 liblightdm-gobject-1-0 liblouis-data liblouis20\n",
      "  liblouisutdml-bin liblouisutdml-data liblouisutdml9 libmessaging-menu0\n",
      "  libmtp-common libmtp-runtime libmtp9 libnfs13 libpaper-utils libpaper1\n",
      "  libpoppler-cpp0v5 libqpdf26 libunity-gtk2-parser0 libunity-gtk3-parser0\n",
      "  libunity-settings-daemon1 liburl-dispatcher1 lightdm nautilus-data\n",
      "  poppler-utils python3-psutil python3-xdg ssl-cert ubuntu-touch-sounds\n",
      "  unity-greeter unity-gtk-module-common unity-gtk2-module unity-gtk3-module\n",
      "  unity-settings-daemon unity-settings-daemon-schemas\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
      "/usr/share/fonts: caching, new cache contents: 0 fonts, 6 dirs\n",
      "/usr/share/fonts/X11: caching, new cache contents: 0 fonts, 4 dirs\n",
      "/usr/share/fonts/X11/Type1: caching, new cache contents: 8 fonts, 0 dirs\n",
      "/usr/share/fonts/X11/encodings: caching, new cache contents: 0 fonts, 1 dirs\n",
      "/usr/share/fonts/X11/encodings/large: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/usr/share/fonts/X11/misc: caching, new cache contents: 89 fonts, 0 dirs\n",
      "/usr/share/fonts/X11/util: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/usr/share/fonts/cMap: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/usr/share/fonts/cmap: caching, new cache contents: 0 fonts, 5 dirs\n",
      "/usr/share/fonts/cmap/adobe-cns1: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/usr/share/fonts/cmap/adobe-gb1: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/usr/share/fonts/cmap/adobe-japan1: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/usr/share/fonts/cmap/adobe-japan2: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/usr/share/fonts/cmap/adobe-korea1: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/usr/share/fonts/opentype: caching, new cache contents: 0 fonts, 1 dirs\n",
      "/usr/share/fonts/opentype/urw-base35: caching, new cache contents: 35 fonts, 0 dirs\n",
      "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 7 dirs\n",
      "/usr/share/fonts/truetype/dejavu: caching, new cache contents: 22 fonts, 0 dirs\n",
      "/usr/share/fonts/truetype/droid: caching, new cache contents: 1 fonts, 0 dirs\n",
      "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
      "/usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs\n",
      "/usr/share/fonts/truetype/noto: caching, new cache contents: 1 fonts, 0 dirs\n",
      "/usr/share/fonts/truetype/quicksand: caching, new cache contents: 4 fonts, 0 dirs\n",
      "/usr/share/fonts/truetype/ubuntu: caching, new cache contents: 14 fonts, 0 dirs\n",
      "/usr/share/fonts/type1: caching, new cache contents: 0 fonts, 1 dirs\n",
      "/usr/share/fonts/type1/urw-base35: caching, new cache contents: 35 fonts, 0 dirs\n",
      "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/root/.local/share/fonts: skipping, no such directory\n",
      "/root/.fonts: skipping, no such directory\n",
      "/usr/share/fonts/X11: skipping, looped directory detected\n",
      "/usr/share/fonts/cMap: skipping, looped directory detected\n",
      "/usr/share/fonts/cmap: skipping, looped directory detected\n",
      "/usr/share/fonts/opentype: skipping, looped directory detected\n",
      "/usr/share/fonts/truetype: skipping, looped directory detected\n",
      "/usr/share/fonts/type1: skipping, looped directory detected\n",
      "/usr/share/fonts/X11/Type1: skipping, looped directory detected\n",
      "/usr/share/fonts/X11/encodings: skipping, looped directory detected\n",
      "/usr/share/fonts/X11/misc: skipping, looped directory detected\n",
      "/usr/share/fonts/X11/util: skipping, looped directory detected\n",
      "/usr/share/fonts/cmap/adobe-cns1: skipping, looped directory detected\n",
      "/usr/share/fonts/cmap/adobe-gb1: skipping, looped directory detected\n",
      "/usr/share/fonts/cmap/adobe-japan1: skipping, looped directory detected\n",
      "/usr/share/fonts/cmap/adobe-japan2: skipping, looped directory detected\n",
      "/usr/share/fonts/cmap/adobe-korea1: skipping, looped directory detected\n",
      "/usr/share/fonts/opentype/urw-base35: skipping, looped directory detected\n",
      "/usr/share/fonts/truetype/dejavu: skipping, looped directory detected\n",
      "/usr/share/fonts/truetype/droid: skipping, looped directory detected\n",
      "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
      "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
      "/usr/share/fonts/truetype/noto: skipping, looped directory detected\n",
      "/usr/share/fonts/truetype/quicksand: skipping, looped directory detected\n",
      "/usr/share/fonts/truetype/ubuntu: skipping, looped directory detected\n",
      "/usr/share/fonts/type1/urw-base35: skipping, looped directory detected\n",
      "/usr/share/fonts/X11/encodings/large: skipping, looped directory detected\n",
      "/var/cache/fontconfig: cleaning cache directory\n",
      "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
      "/root/.fontconfig: not cleaning non-existent cache directory\n",
      "fc-cache: succeeded\n"
     ]
    }
   ],
   "source": [
    "# 한글폰트 설치\n",
    "\n",
    "!sudo apt-get install -y fonts-nanum\n",
    "!sudo fc-cache -fv\n",
    "!rm ~/.cache/matplotlib -rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a78c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글폰트 셋팅\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "import matplotlib as mpl  # 기본 설정 만지는 용도\n",
    "import matplotlib.pyplot as plt  # 그래프 그리는 용도\n",
    "import matplotlib.font_manager as fm  # 폰트 관련 용도\n",
    "\n",
    "plt.rc('font', family='NanumGothic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bdc186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys_font number: 103\n",
      "['/usr/share/fonts/truetype/dejavu/DejaVuSerif-Italic.ttf', '/usr/share/fonts/truetype/liberation/LiberationSerif-BoldItalic.ttf', '/usr/share/fonts/opentype/urw-base35/NimbusSansNarrow-BoldOblique.otf', '/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf', '/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-Italic.ttf', '/usr/share/fonts/truetype/ubuntu/Ubuntu-MI.ttf', '/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-Bold.ttf', '/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-BoldItalic.ttf', '/usr/share/fonts/truetype/ubuntu/UbuntuMono-BI.ttf', '/usr/share/fonts/truetype/nanum/NanumGothicBold.ttf', '/usr/share/fonts/truetype/liberation/LiberationSans-Italic.ttf', '/usr/share/fonts/opentype/urw-base35/NimbusMonoPS-Bold.otf', '/usr/share/fonts/opentype/urw-base35/URWGothic-BookOblique.otf', '/usr/share/fonts/truetype/nanum/NanumGothic.ttf', '/usr/share/fonts/truetype/liberation/LiberationSansNarrow-BoldItalic.ttf', '/usr/share/fonts/opentype/urw-base35/D050000L.otf', '/usr/share/fonts/truetype/nanum/NanumSquareR.ttf', '/usr/share/fonts/opentype/urw-base35/NimbusSansNarrow-Oblique.otf', '/usr/share/fonts/opentype/urw-base35/C059-Bold.otf', '/usr/share/fonts/truetype/dejavu/DejaVuSansMono-BoldOblique.ttf', '/usr/share/fonts/truetype/liberation/LiberationMono-BoldItalic.ttf', '/usr/share/fonts/opentype/urw-base35/NimbusSans-Bold.otf', '/usr/share/fonts/truetype/ubuntu/Ubuntu-M.ttf', '/usr/share/fonts/truetype/quicksand/Quicksand-Bold.ttf', '/usr/share/fonts/truetype/ubuntu/Ubuntu-C.ttf', '/usr/share/fonts/opentype/urw-base35/URWGothic-Book.otf', '/usr/share/fonts/truetype/ubuntu/UbuntuMono-RI.ttf', '/usr/share/fonts/truetype/dejavu/DejaVuSans-Oblique.ttf', '/usr/share/fonts/truetype/ubuntu/Ubuntu-BI.ttf', '/usr/share/fonts/truetype/dejavu/DejaVuSans-BoldOblique.ttf', '/usr/share/fonts/truetype/quicksand/Quicksand-Medium.ttf', '/usr/share/fonts/truetype/nanum/NanumSquareRoundR.ttf', '/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed.ttf', '/usr/share/fonts/opentype/urw-base35/NimbusMonoPS-BoldItalic.otf', '/usr/share/fonts/opentype/urw-base35/NimbusMonoPS-Italic.otf', '/usr/share/fonts/truetype/dejavu/DejaVuSerif-BoldItalic.ttf', '/usr/share/fonts/truetype/droid/DroidSansFallbackFull.ttf', '/usr/share/fonts/opentype/urw-base35/NimbusSansNarrow-Bold.otf', '/usr/share/fonts/truetype/nanum/NanumBarunGothicBold.ttf', '/usr/share/fonts/truetype/dejavu/DejaVuMathTeXGyre.ttf', '/usr/share/fonts/opentype/urw-base35/URWBookman-Light.otf', '/usr/share/fonts/truetype/ubuntu/UbuntuMono-R.ttf', '/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-Oblique.ttf', '/usr/share/fonts/truetype/liberation/LiberationSerif-Bold.ttf', '/usr/share/fonts/opentype/urw-base35/P052-Roman.otf', '/usr/share/fonts/truetype/dejavu/DejaVuSerif.ttf', '/usr/share/fonts/truetype/liberation/LiberationSans-BoldItalic.ttf', '/usr/share/fonts/truetype/nanum/NanumSquareB.ttf', '/usr/share/fonts/opentype/urw-base35/NimbusSans-BoldItalic.otf', '/usr/share/fonts/opentype/urw-base35/NimbusSans-Italic.otf', '/usr/share/fonts/truetype/ubuntu/Ubuntu-LI.ttf', '/usr/share/fonts/truetype/liberation/LiberationMono-Italic.ttf', '/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-Bold.ttf', '/usr/share/fonts/truetype/noto/NotoMono-Regular.ttf', '/usr/share/fonts/opentype/urw-base35/C059-Roman.otf', '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf', '/usr/share/fonts/opentype/urw-base35/P052-Bold.otf', '/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-BoldOblique.ttf', '/usr/share/fonts/truetype/ubuntu/UbuntuMono-B.ttf', '/usr/share/fonts/opentype/urw-base35/NimbusMonoPS-Regular.otf', '/usr/share/fonts/opentype/urw-base35/URWGothic-Demi.otf', '/usr/share/fonts/opentype/urw-base35/P052-BoldItalic.otf', '/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed.ttf', '/usr/share/fonts/truetype/dejavu/DejaVuSerif-Bold.ttf', '/usr/share/fonts/truetype/ubuntu/Ubuntu-B.ttf', '/usr/share/fonts/opentype/urw-base35/Z003-MediumItalic.otf', '/usr/share/fonts/truetype/nanum/NanumSquareRoundB.ttf', '/usr/share/fonts/truetype/quicksand/Quicksand-Light.ttf', '/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Oblique.ttf', '/usr/share/fonts/opentype/urw-base35/StandardSymbolsPS.otf', '/usr/share/fonts/truetype/quicksand/Quicksand-Regular.ttf', '/usr/share/fonts/opentype/urw-base35/P052-Italic.otf', '/usr/share/fonts/opentype/urw-base35/NimbusSans-Regular.otf', '/usr/share/fonts/truetype/liberation/LiberationSerif-Regular.ttf', '/usr/share/fonts/opentype/urw-base35/NimbusRoman-Bold.otf', '/usr/share/fonts/truetype/dejavu/DejaVuSansMono.ttf', '/usr/share/fonts/truetype/ubuntu/Ubuntu-L.ttf', '/usr/share/fonts/truetype/liberation/LiberationMono-Regular.ttf', '/usr/share/fonts/truetype/dejavu/DejaVuSans-ExtraLight.ttf', '/usr/share/fonts/truetype/liberation/LiberationMono-Bold.ttf', '/usr/share/fonts/opentype/urw-base35/C059-BdIta.otf', '/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Italic.ttf', '/usr/share/fonts/opentype/urw-base35/NimbusRoman-Regular.otf', '/usr/share/fonts/opentype/urw-base35/URWBookman-Demi.otf', '/usr/share/fonts/truetype/liberation/LiberationSerif-Italic.ttf', '/usr/share/fonts/opentype/urw-base35/URWBookman-LightItalic.otf', '/usr/share/fonts/opentype/urw-base35/C059-Italic.otf', '/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Bold.ttf', '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf', '/usr/share/fonts/opentype/urw-base35/NimbusRoman-BoldItalic.otf', '/usr/share/fonts/opentype/urw-base35/NimbusSansNarrow-Regular.otf', '/usr/share/fonts/truetype/ubuntu/Ubuntu-R.ttf', '/usr/share/fonts/opentype/urw-base35/NimbusRoman-Italic.otf', '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', '/usr/share/fonts/truetype/nanum/NanumMyeongjo.ttf', '/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf', '/usr/share/fonts/opentype/urw-base35/URWGothic-DemiOblique.otf', '/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', '/usr/share/fonts/truetype/nanum/NanumMyeongjoBold.ttf', '/usr/share/fonts/opentype/urw-base35/URWBookman-DemiItalic.otf', '/usr/share/fonts/truetype/ubuntu/Ubuntu-Th.ttf', '/usr/share/fonts/truetype/ubuntu/Ubuntu-RI.ttf', '/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Bold.ttf']\n",
      "nanum_font number: 10\n"
     ]
    }
   ],
   "source": [
    "sys_font=fm.findSystemFonts()\n",
    "print(f\"sys_font number: {len(sys_font)}\")\n",
    "print(sys_font)\n",
    "\n",
    "nanum_font = [f for f in sys_font if 'Nanum' in f]\n",
    "print(f\"nanum_font number: {len(nanum_font)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c4cd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NanumGothic\n"
     ]
    }
   ],
   "source": [
    "path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'  # 설치된 나눔글꼴중 원하는 폰트의 전체 경로를 가져온다.\n",
    "font_name = fm.FontProperties(fname=path, size=10).get_name()\n",
    "print(font_name)\n",
    "plt.rc('font', family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c40a3b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c322f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54064170",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "            \n",
    "        self.depth = d_model // self.num_heads\n",
    "            \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "            \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "            \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "        \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "            \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "    \t\t\t\t        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "                \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20221ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out\n",
    "\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53a4a488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# Encoder Layer\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn\n",
    "\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb765829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Layer\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57da3d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n",
    "\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f77f575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# Decoder\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db52d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b41e741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genetate_padding_mask 정의 완료\n"
     ]
    }
   ],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "\n",
    "print(\"genetate_padding_mask 정의 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d23d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f891512",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/transformer/data'\n",
    "kor_path = data_dir + \"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir + \"/korean-english-park.train.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cd12103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정제된 corpus 데이터의 수는 78968 개 입니다.\n"
     ]
    }
   ],
   "source": [
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    assert len(kor) == len(eng)\n",
    "    \n",
    "    cleaned_corpus = list(set(zip(kor, eng)))\n",
    "    \n",
    "    return cleaned_corpus\n",
    "    \n",
    "cleaned_corpus = clean_corpus(kor_path, eng_path)\n",
    "print('정제된 corpus 데이터의 수는', len(cleaned_corpus), '개 입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cac3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # 모든 입력은 소문자로 변환\n",
    "    sentence = sentence.lower()\n",
    "    # 알파벳, 문장부호, 한글만 남기고 모두 제거\n",
    "    sentence = re.sub(r\"[^a-zA-Zㄱ-ㅎ가-힣ㅏ-ㅣ.,?!]+\", \" \", sentence)\n",
    "    # 문장부호 양 사이드에 공백 추가\n",
    "    sentence = re.sub(r\"([,.?!])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "232c3177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성\n",
    "def generate_tokenizer(corpus, vocab_size, lang=\"ko\", pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "    # corpus를 받아 txt 파일로 저장\n",
    "    temp_file = os.getenv('HOME') + f'/aiffel/transformer/temp/corpus_{lang}.txt'\n",
    "    \n",
    "    with open(temp_file, 'w') as f:\n",
    "        for row in corpus:\n",
    "            f.write(str(row) + '\\n')\n",
    "    \n",
    "    # Sentencepiece\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f'--input={temp_file} --pad_id={pad_id} --bos_id={bos_id} --eos_id={eos_id} \\\n",
    "        --unk_id={unk_id} --model_prefix=spm_{lang} --vocab_size={vocab_size}'\n",
    "    )\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(f'spm_{lang}.model')\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9f7331e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/transformer/temp/corpus_ko.txt --pad_id=0 --bos_id=1 --eos_id=2         --unk_id=3 --model_prefix=spm_ko --vocab_size=20000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/transformer/temp/corpus_ko.txt\n",
      "  input_format: \n",
      "  model_prefix: spm_ko\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/transformer/temp/corpus_ko.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 78968 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5053325\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1185\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 78967 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 159141 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 78967\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 195707\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 195707 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=83230 obj=12.5937 num_tokens=378610 num_tokens/piece=4.54896\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=70412 obj=11.4418 num_tokens=379924 num_tokens/piece=5.39573\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=52804 obj=11.4471 num_tokens=396862 num_tokens/piece=7.51576\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=52786 obj=11.4137 num_tokens=397193 num_tokens/piece=7.52459\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=39589 obj=11.5538 num_tokens=420668 num_tokens/piece=10.6259\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=39588 obj=11.5172 num_tokens=420682 num_tokens/piece=10.6265\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=29691 obj=11.7108 num_tokens=447000 num_tokens/piece=15.0551\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=29691 obj=11.6693 num_tokens=447004 num_tokens/piece=15.0552\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=22268 obj=11.9068 num_tokens=473397 num_tokens/piece=21.2591\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=22268 obj=11.8609 num_tokens=473392 num_tokens/piece=21.2588\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=22000 obj=11.8793 num_tokens=474461 num_tokens/piece=21.5664\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=22000 obj=11.8768 num_tokens=474459 num_tokens/piece=21.5663\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: spm_ko.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: spm_ko.vocab\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/transformer/temp/corpus_en.txt --pad_id=0 --bos_id=1 --eos_id=2         --unk_id=3 --model_prefix=spm_en --vocab_size=20000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/transformer/temp/corpus_en.txt\n",
      "  input_format: \n",
      "  model_prefix: spm_en\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/transformer/temp/corpus_en.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 78968 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=10661485\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9909% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=29\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999909\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 78956 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 82992 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 78956\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 44562\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 44562 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34535 obj=9.86221 num_tokens=83351 num_tokens/piece=2.41352\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25851 obj=8.00619 num_tokens=83809 num_tokens/piece=3.242\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=21977 obj=7.92346 num_tokens=84668 num_tokens/piece=3.85257\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=21848 obj=7.90465 num_tokens=84910 num_tokens/piece=3.8864\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: spm_en.model\n",
      "trainer_interface.cc(626) LOG(INFO) Savin"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g vocabs: spm_en.vocab\n"
     ]
    }
   ],
   "source": [
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 20000\n",
    "\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for pair in cleaned_corpus:\n",
    "    k, e = pair[0], pair[1]\n",
    "\n",
    "    kor_corpus.append(preprocess_sentence(k))\n",
    "    eng_corpus.append(preprocess_sentence(e))\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "708c15b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NSIDC의 선임연구원인 마크 서리지는 이 연구결과에 대해 “깜짝 놀랐다”고 설명했다.',\n",
       " 'Mark Serreze, senior research scientist at NSIDC, termed the decline \"astounding.')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84e135a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ed1cbaa0b6457baf1fed326cefca51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# 토큰의 길이가 50 이하인 문장만\n",
    "for idx in tqdm_notebook(range(len(kor_corpus))):\n",
    "    src = ko_tokenizer.EncodeAsIds(kor_corpus[idx])\n",
    "    tgt = en_tokenizer.EncodeAsIds(eng_corpus[idx])\n",
    "    \n",
    "    if len(src) <= 50 and len(tgt) <= 50:\n",
    "        src_corpus.append(src)\n",
    "        tgt_corpus.append(tgt)\n",
    "        \n",
    "# 패딩처리를 완료하여 학습용 데이터를 완성 \n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0b23833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 함수 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e78b25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다. \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10958078",
   "metadata": {},
   "source": [
    "## 2. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b97979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98b37756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens], maxlen=enc_train.shape[-1], padding='post')\n",
    "    \n",
    "    print(len(_input))\n",
    "    print(enc_train.shape[-1])\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(_input, output)\n",
    "        \n",
    "        # InvalidArgumentError: In[0] mismatch In[1] shape: 50 vs. 1: [1,8,1,50] [1,8,1,64] 0 0 [Op:BatchMatMulV2]\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(_input, output, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        \n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8a1df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6028e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff = 2048,\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.2,\n",
    "    shared=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55691393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ecb3c9b442e44a5b059e6fb57786ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a vote .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the same time in the area of the town of the town of the area .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the first time , the first time .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the quake was found in the town of the town of the town of the town of the town of the town of the town of the town of the town of the town of the death toll .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d8b41c193542ecbdb92125ec8a0c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a campaign that is a one of obama s campaign .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is the city of city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the coffee is not available .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll from the death toll from the death toll in the death toll .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e5e6241092e4ad195e2a5344a28ba19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the obama is the first time .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is the city siena .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee is not coffee .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll was hit by a rise in the city .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca3c62071df4b57ae49941e86b404ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a third person .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the demonstrators are being used to be in the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee does not mean coffee .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll is hit by a seventh of seven people .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31289cba3db74649971862070b329e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the obama is a man .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is a mountainous city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the memory is no .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven seven of seven seven seven people were killed tuesday .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a14a3203c2411b8eb9fabbeb8da64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: it is the kind of personal .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is in the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there s no more need to be there .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were killed and were wounded .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9fd976876446009845a65c32e8d1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the president is part of the country .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is one of the city s biggest city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee needs to do so .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were confirmed deaths on monday , and seven others were wounded .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184ee1a4f2494788a4651555c19ce47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: that s the president of the country .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is keeping the mountain in a mountain .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: practice has no closer .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven of the seven death toll stood on sunday , on the seventh of the seventh day of the seventh day of the seventh fatality .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "978762efac4140168640dfcf24ed11c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the second leg of his five year old connection\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they are on city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee needs to do away with coffee\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven seven of the seventh fatality in my seven my seven my seven seven seven seven my seventh floor seven days .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f06a2f838d743d6a4aaf2f67e2b9ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: it s the point of that obama .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the only in the city of mumbai .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee needs to do is take at coffee .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven seven of seven seven seven seven seven of the seven seven seven seven seven seven seven seven people were killed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9824612b8924fc19eba5366b299b6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the second of the president .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: so the city is beneath .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: if coffee needs coffee is fading .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven sri lankan seven personnel were among the seven seven seven seven seven seven seven seven of the seven seven seven seven seven seven seven seven seven seven people .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546e3fea6ae34255a37cf360f3e43168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is eighth .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the only city around .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee needs coffee\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven seven sri lankan seven astronauts were killed and seven others .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2870845d657e4655a98d3edfd6f8e5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: that s the president .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the only neighbors in the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee needs at the coffee house .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven others died wednesday seven of seven deaths .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2078a20801f94125a2c019407aa198d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: yes , that would be president of the united states .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: it s the city of to go to the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee needs to do music at the coffee\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the seventh floor leader was killed friday when a seventh personnel died .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0abd73f42df4a97bf45fcea3f53f8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the second life of the younger man .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the only place in the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee needs no coffee right at the coffee\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the seven seven seventh of the seventh vote fourth in the seventh vote related to the seventh vote thursday\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704483fb9ce94837a3a28f89351adc32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the second presidential candidate .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: where the city is a god .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee needs at coffee\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven seven seven others were originally listed in theni province on friday , a seven report said .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0144f9670642f08baabe59bfd44a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the second leg of the oval office .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: where the city is the only one of the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee needs at coffee houses .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven seven seven seven seven seven seven seven seven seven seven seven seven seven seven seven seven seven seven seven seven seven days were taken into the began thursday evening .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a4b7f201df488fb1a2ea60927ba6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: he is a second person in that country .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: where goes to the city .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: if stay there is no coffee .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven others were among the seven seven people .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1f8936a8ea4e41a8180f2298115e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president obama is in very car .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: on sunday the city is a god renowned for god .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: if coffee do is not coffee .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven sri lanka s seven report shows seven of the seven seven seven people were dead and seven others were still vulnerable .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63faf9ce5ff947b288a4b3c1cdf756d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a popular vote on his wife .\n",
      "1\n",
      "50\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the crowd is on the brink of five .\n",
      "1\n",
      "50\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee needs to do is not coffee .\n",
      "1\n",
      "50\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the seven were among the seven casualtiess and a mostly whom died , the fire officials said on sunday .\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "examples = [\n",
    "            \"오바마는 대통령이다.\",\n",
    "            \"시민들은 도시 속에 산다.\",\n",
    "            \"커피는 필요 없다.\",\n",
    "            \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    for example in examples:\n",
    "        translate(example, transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d6ac68",
   "metadata": {},
   "source": [
    "## 3. Project Retrospective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae71990",
   "metadata": {},
   "source": [
    "+ 두 가지 오류로 인해 Transformer 모델 구현 및 프로젝트 완성에 많은 시간이 소요되었으며, LMS -> Colab에서도 나중에야 동작에 문제 없음을 다시 한 번 확인하였다.\n",
    "    - 허무하게도, 원인은 원본 파일이 위치해 있는 (.ko) 곳에 txt 파일이 동시에 위치해서는 안 되는 것이었다. 이유는 공식 문서에서도 찾지 못했다. 폴더를 따로 만들어, 경로를 분리 지정하였더니 해결되었다.\n",
    "    - How to solve OSError: [Errno 30] Read-only file system?\n",
    "        + https://www.kaggle.com/questions-and-answers/70138\n",
    "    - OSError: Not found: unknown field name “xxxx” in TrainerSpec\n",
    "        + https://forums.fast.ai/t/lesson-8-notebook-10-nlp-oserror-not-found-unknown-field-name-minloglevel-in-trainerspec/71436\n",
    "        \n",
    "+ 이번 프로젝트에서는 이전과 동일한 DataSet을 사용하므로 전체적인 윤곽을 잡는 것은 그렇게 어렵지 않았다. 하지만 이전과 달랐던 점은 모델 학습결과 번역 성능이 Attention 대비 월등히 향상되었다는 점이며, 데이터 전처리부터 토크나이징, 패딩 외에 성능을 끌어올리기 위한 다양한 예시 코드들을 보면서 이전에 하던 하이퍼파라미터 튜닝은 참 소소한(?) 작업이었구나 하는 점을 다시금 느낀다.\n",
    "\n",
    "+ 다음 그룹 프로젝트에서 진행하는 것도, 데이터셋만 다를 뿐, 실제로 진행하는 전 과정이 비슷하게 진행될 것으로 보인다. 따라서 이번에 고생한 경험을 발판삼아 조금 덜 고생할 수 있기를 바란다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
